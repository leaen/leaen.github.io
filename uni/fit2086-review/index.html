
<!DOCTYPE html>
<html lang="en-us">
<head>

  
  <meta charset="UTF-8">
  <title>
    Exam questions for FIT2086 | Moss&#39; Blog
  </title>


  
  <meta name="viewport" content="width=device-width,user-scalable=no,maximum-scale=1,initial-scale=1">

  
  <link rel="canonical" href="http://banay.me/uni/fit2086-review/"/>

  
  <link rel="stylesheet" href="/css/sanitize.css">
  <link rel="stylesheet" href="/css/responsive.css">
  <link rel="stylesheet" href="/css/highlight_monokai.css">
  <link rel="stylesheet" href="/css/theme.css">
  <link rel="stylesheet" href="/css/custom.css">
  
  
  <link href="http://banay.me/index.xml" rel="alternate" type="application/rss+xml" title="Moss&#39; Blog" />
  <link href="http://banay.me/index.xml" rel="feed" type="application/rss+xml" title="Moss&#39; Blog" />

  
  


</head>



<body>
<div class="container">

  
  <header role="banner">
    <div class="row gutters">
      <div id="site-title" class="col span_6">
        <h1><a href="http://banay.me/">Moss&#39; Blog</a></h1>
        <h2>CS, Finance, Math</h2>
      </div>
      <div id="social" class="col span_6">
        <ul>
          
          
          
          <li><a href="http://banay.me/index.xml" type="application/rss+xml" target="_blank">RSS</a></li>
        </ul>
      </div>
    </div>
  </header>


  
  <main id="single" role="main">
    <div class="article-header">
      <h1>Exam questions for FIT2086</h1>
      <div class="meta">
        Oct 22, 2018 &nbsp;
        
      </div>
    </div>
    <article>
      

<h2 id="lecture-1">Lecture 1</h2>

<blockquote>
<p>Define the following terms:</p>

<ul>
<li>A population</li>
<li>A sample</li>
<li>A model</li>
</ul>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is <em>inference</em>?</p>
</blockquote>

<h2 id="lecture-2">Lecture 2</h2>

<blockquote>
<p>What is <em>variance</em> in the context of a random variable?</p>
</blockquote>

<h2 id="lecture-3">Lecture 3</h2>

<blockquote>
<p>Briefly explain what maximum likelihood works is and a reason why it may be so effective</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Suppose we have an unbiased estimator \(\hat{\theta}\) of a parameter \(\theta\) with variance \(2.00\). What is the MSE of \(\hat{\theta}\)?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is the sampling distribution of the mean of a random variable \(Y\) with finite mean and variance?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What does it mean for an estimator to be consistent?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is the weak law of large numbers?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>The pmf for a Poisson distribution is given by</p>

<p>\[ p(y \, | \, \lambda) = \frac{\lambda^y \, \text{exp}(- \lambda)}{y!} \]</p>

<p>Derive the negative log likelihood and therefore maximum likelihood estimator for \(Y \sim Poi(\lambda)\).</p>
</blockquote>

<h2 id="lecture-4">Lecture 4</h2>

<blockquote>
<p>What is the central limit theorem?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Suppose I draw a smaple from a distribution and calculate a 95% confidence interval using that sample. What is the probability that the true paramater of the distribution is contained in my CI?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Give the formula for the \(100(1-\alpha)%\) CI of \(\hat{\mu}\) for a normal distribution with known variance \(\sigma^2\). What changes if the variance is <em>unknown</em>?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Appealing to the central limit theorem, give the formula for a CI on the difference in means for two normal distributions where none of the means or variances are known. Why is this an approximation of the true CI? Is there something it does not take into account?</p>
</blockquote>

<h2 id="lecture-5">Lecture 5</h2>

<blockquote>
<p>What is a null hypothesis?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is a test-statistic and a p-value?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Give the formula for the p-value for each of the following hypotheses:</p>

<ul>
<li>\(H_0 : \mu = \mu_0\) vs \(H_A : \mu \not = \mu_0\)</li>
<li>\(H_0 : \mu \leq \mu_0\) vs \(H_A : \mu &gt; \mu_0\)</li>
<li>\(H_0 : \mu \geq \mu_0\) vs \(H_A : \mu &lt; \mu_0\)</li>
</ul>
</blockquote>

<h2 id="lecture-6">Lecture 6</h2>

<blockquote>
<p>Why might we want to use the sum of <em>squared</em> errors as a measure of fit in our model?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Explain the link between least squares estimator and the maxmimum likelihood of \(\beta_0\) and \(\beta\).</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Suppose we have collected a sample of data and fit a multiple linear regression model to the data using 3 predictors for the target. We calculated the RSS to be equal to 300 units. What is the ML and unibiased estimates of the variance of the residuals?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Why is it preferable to use complexity penalties?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Suppose you have collected a sample of 72 measurements on 1531 individuals in Melbourne and are looking to predict their age from that data. Your friend Tim suggests you use the all subsets approach for model selection while your other friend Meg suggests you use a forward selection method. Who do you listen to and why?</p>
</blockquote>

<h2 id="lecture-7">Lecture 7</h2>

<blockquote>
<p>What is the key assumption of Naive Bayes? How does it help reduce the complexity of fitting the model?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Suppose we are approaching a classification problem with \(30\) binary categorical predictors. The target variable has 10 classes.</p>

<ol>
<li>Why can&rsquo;t we use the simplest model directly estimating the joint distribution?</li>
<li>How many probabilites would we need to estimate for Naive Bayes?</li>
</ol>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What &ldquo;shortcut&rdquo; does logistic regression take?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What are <em>sensitivity</em> and <em>specificity</em>?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is AUC?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is logarithmic loss? Why might it be preferred to classification accuracy?</p>
</blockquote>

<h2 id="lecture-8">Lecture 8</h2>

<blockquote>
<p>What is the Bonferroni approach? Why is it used?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Henry has collected a sample of medical imaging data with \(p = 10,000\) and \(n = 524\). Jasmine suggests he use the \(RIC\) information criterion. What might Jasmine&rsquo;s rationale be?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Briefly explain the forward and background selection algorithms for predictor selection.</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Briefly explain what it means for a model to have statistical instability.</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is the difference between linear, ridge and LASSO regression?</p>
</blockquote>

<h2 id="lecture-9">Lecture 9</h2>

<blockquote>
<p>Briefly outline \(k\) fold CV and LOO CV.</p>
</blockquote>

<h2 id="lecture-10">Lecture 10</h2>

<blockquote>
<p>What is unsupervised learning? How does it differ from supervised learning?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>Very briefly outline the steps in the $k$-means algorithm.</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>In what way is mixture modelling an extension of $k$-means?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is the matrix completion problem? Give two examples of when it may be used.</p>
</blockquote>

<h2 id="lecture-11">Lecture 11</h2>

<blockquote>
<p>What is a psuedo-random number generator?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is bootstrapping?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is bagging?</p>
</blockquote>

<!--quoteend-->

<blockquote>
<p>What is a permutation test?</p>
</blockquote>

      
      
      
    </article>
    


  </main>
  
  <nav class="pagination-single">
    
      <span class="previous">&larr; <a href="http://banay.me/post/aho-corasick/" rel="prev">Fast keyword matching with the Aho-Corasick algorithm</a></span>
    
    
      <span class="next"><a href="http://banay.me/uni/fit2014-review/" rel="next">Exam questions for FIT2014</a> &rarr;</span>
    
  </nav>


  
  <footer role="contentinfo">
    <div style="text-align:center;">
      
      
    </div>
  </footer>


</div>

<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-110997704-1', 'auto');
	ga('send', 'pageview');
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>


</body>
</html>

