<!doctype html>
<html lang="en">
<head>
<title>ETC3460 - Financial Econometrics</title>
<!-- 2019-06-06 Thu 12:18 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="generator" content="Org-mode">
<meta name="author" content="Moss Ebeling - Semester 1 2019">

<link  href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/js/bootstrap.min.js"></script>
<style type="text/css">
/* org mode styles on top of twbs */

html {
    position: relative;
    min-height: 100%;
}

body {
    font-size: 18px;
    margin-bottom: 105px;
}

footer {
    position: absolute;
    bottom: 0;
    width: 100%;
    height: 101px;
    background-color: #f5f5f5;
}

footer > div {
    padding: 10px;
}

footer p {
    margin: 0 0 5px;
    text-align: center;
    font-size: 16px;
}

#table-of-contents {
    margin-top: 20px;
    margin-bottom: 20px;
}

blockquote p {
    font-size: 18px;
}

pre {
    font-size: 16px;
}

.footpara {
    display: inline-block;
}

figcaption {
  font-size: 16px;
  color: #666;
  font-style: italic;
  padding-bottom: 15px;
}

/* from twbs docs */

.bs-docs-sidebar.affix {
    position: static;
}
@media (min-width: 768px) {
    .bs-docs-sidebar {
        padding-left: 20px;
    }
}

/* All levels of nav */
.bs-docs-sidebar .nav > li > a {
    display: block;
    padding: 4px 20px;
    font-size: 14px;
    font-weight: 500;
    color: #999;
}
.bs-docs-sidebar .nav > li > a:hover,
.bs-docs-sidebar .nav > li > a:focus {
    padding-left: 19px;
    color: #A1283B;
    text-decoration: none;
    background-color: transparent;
    border-left: 1px solid #A1283B;
}
.bs-docs-sidebar .nav > .active > a,
.bs-docs-sidebar .nav > .active:hover > a,
.bs-docs-sidebar .nav > .active:focus > a {
    padding-left: 18px;
    font-weight: bold;
    color: #A1283B;
    background-color: transparent;
    border-left: 2px solid #A1283B;
}

/* Nav: second level (shown on .active) */
.bs-docs-sidebar .nav .nav {
    display: none; /* Hide by default, but at >768px, show it */
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 30px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav > li > a:focus {
    padding-left: 29px;
}
.bs-docs-sidebar .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav > .active:focus > a {
    padding-left: 28px;
    font-weight: 500;
}

/* Nav: third level (shown on .active) */
.bs-docs-sidebar .nav .nav .nav {
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 40px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav .nav > li > a:focus {
    padding-left: 39px;
}
.bs-docs-sidebar .nav .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav .nav > .active:focus > a {
    padding-left: 38px;
    font-weight: 500;
}

/* Show and affix the side nav when space allows it */
@media (min-width: 992px) {
    .bs-docs-sidebar .nav > .active > ul {
        display: block;
    }
    /* Widen the fixed sidebar */
    .bs-docs-sidebar.affix,
    .bs-docs-sidebar.affix-bottom {
        width: 213px;
    }
    .bs-docs-sidebar.affix {
        position: fixed; /* Undo the static from mobile first approach */
        top: 20px;
    }
    .bs-docs-sidebar.affix-bottom {
        position: absolute; /* Undo the static from mobile first approach */
    }
    .bs-docs-sidebar.affix .bs-docs-sidenav,.bs-docs-sidebar.affix-bottom .bs-docs-sidenav {
        margin-top: 0;
        margin-bottom: 0
    }
}
@media (min-width: 1200px) {
    /* Widen the fixed sidebar again */
    .bs-docs-sidebar.affix-bottom,
    .bs-docs-sidebar.affix {
        width: 263px;
    }
}
</style>
<script type="text/javascript">
$(function() {
    'use strict';

    $('.bs-docs-sidebar li').first().addClass('active');

    $(document.body).scrollspy({target: '.bs-docs-sidebar'});

    $('.bs-docs-sidebar').affix();
});
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  displayAlign: "center",
  displayIndent: "2em",
  messageStyle: "none",
  "HTML-CSS": {
    scale: 100,
    styles: {
      ".MathJax_Display": {
        "font-size": "100%"
      }
    }
  },
  "SVG": {
    scale: 100,
    styles: {
      ".MathJax_SVG_Display": {
        "font-size": "100%",
        "margin-left": "-2.281em"
      }
    }
  }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
</head>
<body>
<div id="content" class="container">
<div class="row"><div class="col-md-9"><h1 class="title">ETC3460 - Financial Econometrics</h1>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Unit guide summary</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Synopsis</h3>
<div class="outline-text-3" id="text-1-1">
<p>
The specification, estimation and testing of asset pricing models, including the capital asset pricing model and extensions; the statistical characteristics of financial data emphasising skewness, kurtosis and volatility aspects; volatility models such as ARCH models of financial time series, with applications to stock prices, derivatives, and exchange rates including the forecast performance of these models.
</p>
</div>
</div>

<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Assessment summary</h3>
<div class="outline-text-3" id="text-1-2">
<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="right">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Assessment task</th>
<th scope="col" class="text-right">Value</th>
<th scope="col" class="text-left">Due date</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">Written assessment 1</td>
<td class="text-right">15%</td>
<td class="text-left">Week 6</td>
</tr>

<tr>
<td class="text-left">Written assessment 2</td>
<td class="text-right">15%</td>
<td class="text-left">Week 12</td>
</tr>

<tr>
<td class="text-left">Group assignment</td>
<td class="text-right">10%</td>
<td class="text-left">Week 11, 12</td>
</tr>

<tr>
<td class="text-left">Final examination</td>
<td class="text-right">60%</td>
<td class="text-left">TBA</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Unit schedule</h3>
<div class="outline-text-3" id="text-1-3">
<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="right">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-right">Week</th>
<th scope="col" class="text-left">Topic</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-right">1</td>
<td class="text-left">Background statistics</td>
</tr>

<tr>
<td class="text-right">2</td>
<td class="text-left">Returns and their statistical properties</td>
</tr>

<tr>
<td class="text-right">3</td>
<td class="text-left">Modelling returns</td>
</tr>

<tr>
<td class="text-right">4</td>
<td class="text-left">Asset pricing I</td>
</tr>

<tr>
<td class="text-right">5</td>
<td class="text-left">Asset pricing II</td>
</tr>

<tr>
<td class="text-right">6</td>
<td class="text-left">Efficient market hypothesis</td>
</tr>

<tr>
<td class="text-right">7</td>
<td class="text-left">Modelling predictable returns</td>
</tr>

<tr>
<td class="text-right">8</td>
<td class="text-left">Introduction to volatility modelling</td>
</tr>

<tr>
<td class="text-right">9</td>
<td class="text-left">ARCH model</td>
</tr>

<tr>
<td class="text-right">10</td>
<td class="text-left">Testing for ARCH effects</td>
</tr>

<tr>
<td class="text-right">11</td>
<td class="text-left">GARCH modelling and extensions</td>
</tr>

<tr>
<td class="text-right">12</td>
<td class="text-left">Presentations and unit review</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Lectures</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Lecture 1</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1"><span class="section-number-4">2.1.1</span> Topic outlines</h4>
<div class="outline-text-4" id="text-2-1-1">
<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Topic</th>
<th scope="col" class="text-left">Lecturer</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">Basic statistics and probability</td>
<td class="text-left">David Frazier</td>
</tr>

<tr>
<td class="text-left">Asset Pricing I</td>
<td class="text-left">David Frazier</td>
</tr>

<tr>
<td class="text-left">Asset Pricing II</td>
<td class="text-left">David Frazier</td>
</tr>

<tr>
<td class="text-left">Modelling predictable returns</td>
<td class="text-left">David Frazier</td>
</tr>

<tr>
<td class="text-left">Modelling asset volatility</td>
<td class="text-left">David Frazier</td>
</tr>

<tr>
<td class="text-left">Revision lectures</td>
<td class="text-left">David Frazier</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2"><span class="section-number-4">2.1.2</span> Motivation and overview</h4>
<div class="outline-text-4" id="text-2-1-2">
<blockquote>
<p>
Financial econometrics is about making decisions in terms of investments and how to maximise the returns of those investments
</p>

<ul class="org-ul">
<li>How do you model exchange rates?
</li>
<li>How do you predict the future value of assets based on the vast volumes of data available?
</li>
</ul>
</blockquote>

<p>
Why is this possible with financial data? Financial data exhibits "universally agreed upon behaviours" (Cont, 2000), including:
</p>
<ul class="org-ul">
<li>Heavy tailed distributions
</li>
<li>Asymmetrical distributions
</li>
<li>Lack of persistence in returns
</li>
<li>Volatility clustering
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Lecture 2 - Probability and statistics review</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> General notes</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
For more information on this topic see <a href="https://www.kevinsheppard.com/images/b/b4/Chapter1.pdf">Kevin Sheppard's notes</a>.
</p>
</div>
</div>

<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> Terminology review</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
A <i>random variable</i> is a phenomena that cannot be predicted with perfect accuracy.
</p>

<p>
A <i>sample space</i>, often denoted \(\Omega\), is the set of all possible values that a random variable may take.
</p>

<p>
An <i>event</i> \(\omega\) is a subset of the sample space. The set of all events in the sample space \(\Omega\) is called the <i>event space</i> and often denoted \(\mathcal{F}\). We can assign probabilities to events using functions that satisfy the probability axioms. Lastly the set of all posible events is called the <i>power set</i> and often denoted \(2^\Omega\).
</p>
</div>
</div>

<div id="outline-container-sec-2-2-3" class="outline-4">
<h4 id="sec-2-2-3"><span class="section-number-4">2.2.3</span> Discrete and continuous random variables</h4>
<div class="outline-text-4" id="text-2-2-3">
<p>
A random variable is essentially a map \(X:\Omega \rightarrow \mathbb{R}\) that takes an element in the sample space to a number on the real line.
</p>

<p>
A <b><b>discrete random variable</b></b> is a random variable for which the cardinality of \(\Omega\) is finite or countably infinite. Conversely, a <b><b>continuous random variable</b></b> may have \(\Omega\) with uncountable cardinality.
</p>

<p>
For continuous random variables the probability of drawing a particular outcome is zero since there are uncountably many possibilities. We can only assign probabilities to subsets of \(\Omega\).
</p>
</div>
</div>

<div id="outline-container-sec-2-2-4" class="outline-4">
<h4 id="sec-2-2-4"><span class="section-number-4">2.2.4</span> Questions</h4>
<div class="outline-text-4" id="text-2-2-4">
<blockquote>
<p>
Suppose \(X:\Omega \rightarrow \mathbb{R}\) is a random variable. What does \(X(\omega)\) represent?
</p>
</blockquote>

<p>
The random variable is a map from the sample space \(\Omega\) to a real number representing the mass in the case of discrete random variables and the density in the case of continuous random variables. We choose \(\mathbb{R}\) as the codomain instead of \([0,1]\) since for continuous random variables the density is often greater than 1. For example let \(X:[0, \frac{1}{2}] \rightarrow \mathbb{R}\) be the continuous random variable over the interval \([0, \frac{1}{2}]\).
</p>

<p>
\[ X(x) = \frac{1}{1 - \frac{1}{2}} = 2 \]
</p>

<blockquote>
<p>
Why do we often use log returns instead of unscaled returns?
</p>
</blockquote>

<p>
First we might answer why we use <i>returns</i> instead of the absolute changes in prices. Suppose we have a large bank stock that increases from $500 to $510. Given the size of the stock this is a relatively small move. However assume instead we saw the same $10 upward move in a stock previously trading for $10. In that case it represents a doubling in the value of the stock (!). Using returns provides <i>normalisation</i> between assets and allows us to compare the rescaled magnitude of moves rather than their absolute value.
</p>

<p>
The motivation for using log returns involves a number of points. More information can also be found on the blog post <a href="https://quantivity.wordpress.com/2011/02/21/why-log-returns/">Why log returns?</a> by Quantivity.
</p>
</div>

<ol class="org-ol"><li><a id="sec-2-2-4-1" name="sec-2-2-4-1"></a>Log-normality<br ><div class="outline-text-5" id="text-2-2-4-1">
<p>
It's a common assumption that returns are approximately log-normally distributed. When working with log returns we therefore have normally distributed values.
</p>
</div>
</li>

<li><a id="sec-2-2-4-2" name="sec-2-2-4-2"></a>Time additivity<br ><div class="outline-text-5" id="text-2-2-4-2">
<p>
When calculating cumulative returns we can simply add returns instead of multiplying them together. This has the benefit that the sum of normals is also normal and we have improved numerical stability since multiplication of small numbers can be problematic.
</p>
</div>
</li>

<li><a id="sec-2-2-4-3" name="sec-2-2-4-3"></a>Approximate raw-log equality<br ><div class="outline-text-5" id="text-2-2-4-3">
<p>
For values close to zero, log returns approximate regular returns. Concretely for \(r \ll 1\)
</p>

<p>
\[ \log(1+r) \approx r \]
</p>

<p>
Some more information for why this occurs can be found on <a href="https://stats.stackexchange.com/questions/244199/why-is-it-that-natural-log-changes-are-percentage-changes-what-is-about-logs-th">this stackoverflow post</a>.
</p>
</div>
</li></ol>
</div>
</div>
<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Lecture 3 - Probability and statistics review</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1"><span class="section-number-4">2.3.1</span> Continuous random variables</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
A random variable \(X:\mathbb{R} \rightarrow \mathbb{R}\) is <i>continuous</i> if it is <i>weakly positive</i> (i.e. \(X(\omega) \geq 0\)) and if the integral over the full sample space is equal to 1 (i.e. \(\int_{\Omega} X(\omega) d \omega = 1\)).
</p>

<p>
The function that maps from the sample space \(\Omega\) to \(\mathbb{R}\) is called the <b>probability density function</b> or <b>pdf</b>. Continuous variables also have an un-countably infinite number of elements in their range \(\Omega\).
</p>
</div>
</div>

<div id="outline-container-sec-2-3-2" class="outline-4">
<h4 id="sec-2-3-2"><span class="section-number-4">2.3.2</span> Normality of asset returns</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
We will come to see that asset returns are only <i>conditionally</i> normal (given their recent history). Unconditionally, we cannot use CLT or normality assumption in our asset returns since this is likely to be violated. Here we look at the distribution of standardised monthly returns on the SP500 compared to a standard normal \(\mathcal{N}(0,1)\).
</p>


<figure>
<p><img src="./images/sp_500_normality.png" class="img-responsive" alt="sp_500_normality.png">
</p>
</figure>

<p>
We see that there is <i>excess probability mass</i> in the tails of the returns of the green line (1870-2015) and the red line (2000-2015). These thicker tails compared to the normal are indicative of <i>positive excess kurtosis</i>, which we will see is the fourth normalised moment of the distribution.
</p>

<p>
Dispite the central limit theorem (sample averages approximate a normal asymptotically), even monthly returns for SP500 over a very long time frame fail to approach a normal distribution.
</p>
</div>
</div>

<div id="outline-container-sec-2-3-3" class="outline-4">
<h4 id="sec-2-3-3"><span class="section-number-4">2.3.3</span> Raw, central and normalised moments</h4>
<div class="outline-text-4" id="text-2-3-3">
<p>
Concretely, the \(k\) -th <b>moment</b> of a random variable \(X\) is denoted \(\mu_k\) and equal to
</p>

<p>
\[ \mu_k = \mathbb{E}[X^k] \]
</p>

<p>
Since this expectation is affected by the level and spread of the distribution, we may also be interested in \(\bar{\mu}_k\) the \(k\) -th <b>central moment</b> \(\bar{\mu}_k\)
</p>

<p>
\[ \bar{\mu}_k = \mathbb{E}[(X - \mu_1)^k] \]
</p>

<p>
or the \(k\) -th <b>normalised moment</b> \(\bar{\mu}_k^s\)
</p>

<p>
\[ \bar{\mu}_k^s = \mathbb{E} \left [ \left (\frac{X - \mu_1}{\sqrt{\bar{\mu}_2}} \right)^k \right ] \]
</p>

<p>
There are some familar names for the first few moments of a distribution. Below we show these along with some analogous financial concepts.
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="right">

<col  class="left">

<col  class="left">

<col  class="left">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-right">\(k\)</th>
<th scope="col" class="text-left">Raw moment</th>
<th scope="col" class="text-left">Central moment</th>
<th scope="col" class="text-left">Normalised moment</th>
<th scope="col" class="text-left">Financial interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-right">1</td>
<td class="text-left">Mean</td>
<td class="text-left">-</td>
<td class="text-left">-</td>
<td class="text-left">Average return</td>
</tr>

<tr>
<td class="text-right">2</td>
<td class="text-left">-</td>
<td class="text-left">Variance</td>
<td class="text-left">-</td>
<td class="text-left">Risk</td>
</tr>

<tr>
<td class="text-right">3</td>
<td class="text-left">-</td>
<td class="text-left">-</td>
<td class="text-left">Skewness</td>
<td class="text-left">Whether extreme returns are above or below the average</td>
</tr>

<tr>
<td class="text-right">4</td>
<td class="text-left">-</td>
<td class="text-left">-</td>
<td class="text-left">Kurtosis</td>
<td class="text-left">Likelihood of extreme returns</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-sec-2-3-4" class="outline-4">
<h4 id="sec-2-3-4"><span class="section-number-4">2.3.4</span> Questions</h4>
<div class="outline-text-4" id="text-2-3-4">
<blockquote>
<p>
What is <i>equidispersion</i>?
</p>
</blockquote>

<p>
A distribution of a random variable \(X\) is said to exhibit equidispersion when
</p>

<p>
\[ \mathbb{E}[X] = \mathbb{V}(X) \]
</p>

<p>
An examples of a distribution that has equidispersion is the Poisson distribution since if \(X \sim \mathcal{P}(\lambda)\) then \(\mathbb{E}[X] = \lambda = \mathbb{V}(X)\). Simply, the mean and variance are equal.
</p>

<blockquote>
<p>
What is <i>excess kurtosis</i>? How does it differ from <i>kurtosis</i>?
</p>
</blockquote>

<p>
Raw kurtosis is difficult to interpret when we have nothing to compare it to. A common solution to this is <i>excess kurtosis</i> which compares the kurtosis of a distribution with that of a standard normal. Excess kurtosis is equal to \(\bar{\mu}_k^s - 3\), since a standard normal has raw kurtosis of 3.
</p>

<p>
Positive excess kurtosis indicates that a distribution tends to have thicker tails than a standard normal (leptokurtic) and therefore is more likely to have extreme values and negative excess kurtosis indicates thinner tails (platykurtic) and is less likely to have extreme values.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> Lecture 4 - Multivariate distributions</h3>
<div class="outline-text-3" id="text-2-4">
</div>
<div id="outline-container-sec-2-4-1" class="outline-4">
<h4 id="sec-2-4-1"><span class="section-number-4">2.4.1</span> Sample skewness</h4>
<div class="outline-text-4" id="text-2-4-1">
<p>
We can estimate the third standardised moment \(\hat{\mu}^s_3\) or <i>skewness</i> by
</p>

<p>
\[ \hat{\mu}^s_3 = \frac{1}{T} \sum_{i=1}^T \left ( \frac{r_t - \bar{r}}{s} \right )^3 \]
</p>

<p>
Skewness tells us nothing about the magnitude or likelihood of positive or negative events. If skewness is positive then we have right skew and if it is negative we have left skew.
</p>
</div>
</div>

<div id="outline-container-sec-2-4-2" class="outline-4">
<h4 id="sec-2-4-2"><span class="section-number-4">2.4.2</span> Sample kurtosis</h4>
<div class="outline-text-4" id="text-2-4-2">
<p>
We can estimate the four standardised moment \(\hat{\mu}^s_3\) or <i>kurtosis</i> by
</p>

<p>
\[ \hat{\mu}^s_4 = \frac{1}{T} \sum_{i=1}^T \left ( \frac{r_t - \bar{r}}{s} \right )^4 \]
</p>

<p>
We use the kurtosis of the standard normal distribution as a benchmark for kurtosis. We call this adjusted value <i>excess kurtosis</i>.
</p>

<p>
\[ \text{Excess kurtosis} = \text{Kurtosis} - 3 \]
</p>

<p>
Positive excess kurtosis indicates that the tails of a distribution are thicker than that of a normal. Negative excess kurtosis indicates thinner tails (e.g. binomial distribution).
</p>
</div>
</div>

<div id="outline-container-sec-2-4-3" class="outline-4">
<h4 id="sec-2-4-3"><span class="section-number-4">2.4.3</span> Joint distributions</h4>
<div class="outline-text-4" id="text-2-4-3">
<p>
Similar to the definitions we've given for continuous univariate random variables (weakly positive and integrates to 1), a multivariate random variables is defined as follows Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space, then \(X:\Omega \to \mathbb{R}^n\) is an \(n\) dimensional random variable. We say \(X\) is continuous if:
</p>

<ol class="org-ol">
<li>Its range \(R(X)\) is uncountably infinite
</li>
<li>\(X\) is weakly positive, i.e.
\[ \mathbb{P}(\omega) = \int \cdots \int_{(x_1, \ldots, x_n) \in \omega} f(x_1, \ldots, x_n) \, dx_1 \, \cdots \, dx_n \geq 0 \text{, for all } \omega \in \Omega \]
</li>
<li>Events outside of the range have probability zero
\[ \mathbb{P}(\omega') = \int \cdots \int_{(x_1, \ldots, x_n) \in \omega'} f(x_1, \ldots, x_n) \, dx_1 \, \cdots \, dx_n = 0 \text{, for all } \omega' \in \bar{\Omega} \]
</li>
<li>The sample space integrates to 1
\[ \mathbb{P}(\Omega) = \int \cdots \int_{(x_1, \ldots, x_n) \in \Omega} f(x_1, \ldots, x_n) \, dx_1 \, \cdots \, dx_n = 1 \]
</li>
</ol>

<p>
Here \(f:\mathbb{R}^n \to \mathbb{R}\) is a <b>multivariate probability density function</b> (pdf) which is said to describe the <b>joint distribution</b> between the \(n\) random variables.
</p>

<p>
For example, let \(X_1, X_2\) be normal distributions with means \(\mu_1, \mu_2\) and variances \(\sigma_1^2, \sigma_2^2\). Then the vector \(\mathbf{X} = (X_1, X_2)^{\prime}\) is <i>bivariate normal</i> with correlation \(\rho\) iff
</p>

<p>
\[ f(x_1, x_2) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \times \exp \left ( -\frac{1}{2 \sqrt{1 - \rho^2}} \left [\frac{(x_1 - \mu_1)^2}{\sigma_1} + \frac{(x_2 - \mu_2)^2}{\sigma_2} - \frac{2 \rho (x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1 \sigma_2} \right ] \right ) \]
</p>

<p>
We can "recover" the marginal distributions for the individual random variables by integrating over the other random variables. For example in the bivariate case above, we can obtain the <i>marginal pdf</i> for \(X_1\) by
</p>

<p>
\[ f(x_1) = \int_{-\infty}^{\infty} f(x_1, x_2) \, dx_2 \]
</p>
</div>
</div>

<div id="outline-container-sec-2-4-4" class="outline-4">
<h4 id="sec-2-4-4"><span class="section-number-4">2.4.4</span> Conditional relationship between bivariate normal distributions</h4>
<div class="outline-text-4" id="text-2-4-4">
<p>
Let \(\mathbf{X} = (X_1, X_2)^{\prime}\) be a bivariate normal distribution
</p>

<p>
\[ \mathbf{X} \sim \mathcal{N} \left (\begin{pmatrix}\mu_1 \\ \mu_2 \end{pmatrix}, \begin{bmatrix}\sigma_{11}^2 & \sigma_{12} \\ \sigma_{21} & \sigma_{22}^2\end{bmatrix} \right ) \]
</p>

<p>
we also note that we could write the covariance matrix as
</p>

<p>
\[ \begin{bmatrix}\sigma_{1}^2 & \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \]
</p>

<p>
where
</p>

<p>
\[ \rho = \frac{\sigma_{12}}{\sigma_1 \sigma_2} = \frac{\sigma_{12}}{\sqrt{\sigma_{11}^2 \sigma_{22}^2}} \]
</p>

<p>
Often we are interested in conditional distributions in the context of financial econometrics. Suppose we are interested in the distribution of \(X_1\) given \(X_2 = x_2\). We find that this is also normally distributed, specifically
</p>

<p>
\[ f(X_1 \, | \, X_2 = x_2) \sim \mathcal{N} \left ( \mu_1 + \rho \frac{\sigma_{11}}{\sigma_{22}}(x_2 - \mu_2), \, \sigma_{11}^2 (1 - \rho^2) \right ) \]
</p>

<p>
If we look closely, this is fairly straightforward. Marginally, \(X_1\) would have a mean of \(\mu_2\) but we adjust it since we know that \(X_1\) and \(X_2\) are correlated with correlation \(\rho\). We calculate how far \(x_2\) is from its mean \(\mu_2\) and rescale that using \(\frac{\sigma_{11}}{\sigma_{22}}\) and finally multiple by \(\rho\), the strength of this correlation. We adjust the unconditional variance \(\sigma_{11}^2\) by the strength of the correlation also. For very strength correlation (\(\mid \rho \mid \gtrapprox 0.7\)) we should shrink the variance by half, while for very low correlation (\(\mid \rho \mid \lessapprox 0.2\)) we would only shrink it by 5%.
</p>

<p>
Understanding this relationship is critical.
</p>
</div>
</div>

<div id="outline-container-sec-2-4-5" class="outline-4">
<h4 id="sec-2-4-5"><span class="section-number-4">2.4.5</span> Conditional expectations</h4>
<div class="outline-text-4" id="text-2-4-5">
<p>
A conditional expectation is simply an expectation in which we integrate over the conditionally distribution rather than the marginal distribution.
</p>

<p>
For example,
</p>

<p>
\[ \mathbb{E}[X_1 \, | \, X_2 = x_2] = \int x_1 f(x_1 \, | \, X_2 = x_2) \, dx_1 \]
</p>
</div>
</div>

<div id="outline-container-sec-2-4-6" class="outline-4">
<h4 id="sec-2-4-6"><span class="section-number-4">2.4.6</span> Relationship between simple linear regression and bivariate normals</h4>
<div class="outline-text-4" id="text-2-4-6">
<p>
Let's walk through a simple derivation of the values of the coefficients in simple linear regression. We want to find parameters that minimize the sum of squared errors. That is,
</p>

<p>
\[ \begin{array}{rcl}
\displaystyle J(\hat{\beta}_0, \hat{\beta}_1) &=& \displaystyle \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \\
\displaystyle \frac{\partial J}{\partial \hat{\beta}_0} &=& \displaystyle 2 \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)(-1) = 0 \\
&=& \displaystyle \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
n \hat{\beta}_0 &=& \displaystyle \sum_{i=1}^n y_i - \hat{\beta}_1 \sum_{i=1}^n x_i \\
\therefore \hat{\beta}_0 &=& \displaystyle \frac{1}{n} \sum_{i=1}^n y_i - \hat{\beta}_1 \frac{1}{n} \sum_{i=1}^n x_i = \bar{y} - \hat{\beta}_1 \bar{x} \\
\end{array} \]
</p>

<p>
Skipping some substitution we can also derive that
</p>

<p>
\[ \displaystyle \hat{\beta}_1 = \displaystyle \frac{\sigma_{12}}{\sigma_{x}^2} \]
</p>

<p>
Rewriting our predictions we reach a familiar form
</p>

<p>
\[ \begin{array}{rcl}
\hat{y}_i &=& \hat{\beta}_0 + \hat{\beta}_1 z_i \\
&=& \bar{y} + \hat{\beta}_1 (z_i - \bar{z}) \\
&=& \hat{\mu}_y + \frac{\hat{\sigma}_{12}}{\hat{\sigma}_z^2} (z_i - \hat{\mu}_z) \\
&=& \hat{\mu}_y + \hat{\rho} \frac{\hat{\sigma}_y}{\hat{\sigma}_z} (z_i - \hat{\mu}_z) \\
\end{array} \]
</p>

<p>
with
</p>

<p>
\[ \hat{\rho} = \frac{\hat{\sigma}_{yz}}{\sqrt{\hat{\sigma}_y^2 \hat{\sigma}_z^2}} \]
</p>

<p>
and therefore we end up with a bivariate normal. This comes directly as a result of the assumptions made in linear regression. This is an important consideration to make in the context of finance since we often work with non-normal distributions, so we should be cautious when making this assumption unknowingly.
</p>
</div>
</div>

<div id="outline-container-sec-2-4-7" class="outline-4">
<h4 id="sec-2-4-7"><span class="section-number-4">2.4.7</span> Law of iterated expectations</h4>
<div class="outline-text-4" id="text-2-4-7">
<p>
Let \(\mathbf{X} = (X_1, X_2)^{\prime}\) be a bivariate distribution with pdf \(f(x_1, x_2)\) and marginal pdfs \(f_1(x_1)\) and \(f_2(x_2)\). Then the law of iterated expectations states that
</p>

<p>
\[ \mathbb{E}[g(X_1)] = \mathbb{E}_{X_2}[ \mathbb{E}_{X_1}[g(X_1) | X_2 = x_2] ] \]
</p>

<p>
The "inner" expectation is made with respect to \(X_1\) while the "outer" expectation is made with respect to \(X_2\). This shows that we can work the conditional and marginal distributions and avoid the joint distribution (which is often more difficult to model).
</p>
</div>
</div>

<div id="outline-container-sec-2-4-8" class="outline-4">
<h4 id="sec-2-4-8"><span class="section-number-4">2.4.8</span> Law of total variance</h4>
<div class="outline-text-4" id="text-2-4-8">
<p>
Conditional variance is defined in the same way as normal variance. That is the expected squared distance from the averaged. Concretely, this look like
</p>

<p>
\[ \mathbb{V}(Y | X) = \mathbb{E}[(Y - \mathbb{E}[Y|X])^2 | X] \]
</p>

<p>
Using some algebra, we can show that
</p>

<p>
\[ \begin{array}{rcl}
\mathbb{V}(Y) &=& \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 \\
&=& \mathbb{E}[\mathbb{E}[Y^2|X]] - \mathbb{E}[\mathbb{E}[Y|X]]^2 \\
&=& \mathbb{E}[\mathbb{V}(Y|X) + \mathbb{E}[Y|X]^2] - \mathbb{E}[\mathbb{E}[Y|X]]^2 \\
&=& \mathbb{E}[\mathbb{V}(Y|X)] + \mathbb{E}[\mathbb{E}[Y|X]^2] - \mathbb{E}[\mathbb{E}[Y|X]]^2 \\
&=& \mathbb{E}[\mathbb{V}(Y|X)] + \mathbb{V}(\mathbb{E}[Y|X]) \\
\end{array} \]
</p>

<p>
This is the law of total variance.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5"><span class="section-number-3">2.5</span> Lecture 5 - Financial assets and returns</h3>
<div class="outline-text-3" id="text-2-5">
</div>
<div id="outline-container-sec-2-5-1" class="outline-4">
<h4 id="sec-2-5-1"><span class="section-number-4">2.5.1</span> Fixed-income securities and equities</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
Fixed-income products are designed as a lump sum payment at some future date, sometimes combined with coupon payments (interest payments) paid out at regular intervals. Equities on the other hand are shares or stakes in a company and therefore a claim on the assets and future cashflows of a firm. Both of these can be traded, although typically equities are more often traded on exchanges while fixed-income is usually traded over the counter (OTC) between parties.
</p>
</div>
</div>

<div id="outline-container-sec-2-5-2" class="outline-4">
<h4 id="sec-2-5-2"><span class="section-number-4">2.5.2</span> Returns</h4>
<div class="outline-text-4" id="text-2-5-2">
<p>
We generally work with three different types of returns. Net, gross and log returns. <b>Net returns</b> (commonly denoted \(R_t\)) is the <b>percentage change</b> over a period. The net return over the period from \(t-1\) to \(t\) can be simply found by
</p>

<p>
\[ R_t = \frac{P_t - P_{t-1}}{P_{t-1}} \]
</p>

<p>
Gross returns are simply the multiple corresponding to the percentage change
</p>

<p>
\[ 1 + R_t = 1 + \frac{P_t - P_{t-1}}{P_{t-1}} = \frac{P_t}{P{t-1}} \]
</p>

<p>
<b>Gross returns</b> are <i>multiplicative</i>, meaning that if we have the gross returns for each step over a period we can multiply them together to get the multi-period return for that period.
</p>

<p>
\[ \begin{array}{rcl}
R_t(k) &=& \displaystyle \frac{P_t}{P_{t-k}} -1 \\
&=& (1+R_t) \times (1+R_{t-1}) \times \cdots \times (1+R_{t-k+1}) \\
&=& \displaystyle \prod_{j=0}^{k-1} (1 + R_{t-j}) - 1 \\
\end{array} \]
</p>

<p>
Lastly, continuously compounded returns or <b>log returns</b> are equal to the log of gross returns
</p>

<p>
\[ r_t = \log(1+R_t) \]
</p>

<p>
Log returns have the benefit of being <i>additive</i>. The \(k\) period log return is simply equal to
</p>

<p>
\[ \begin{array}{rcl}
r_t(k) &=& \log(1 + R_t(k)) \\
&=& \log [ (1+R_t) \times (1+R_{t-1}) \times \cdots \times (1+R_{t-k+1}) ] \\
&=& \log (1+R_t) + \log(1+R_{t-1}) + \cdots + \log(1+R_{t-k+1}) \\
&=& r_t + r_{t-1} + \cdots + r_{t-k+1} \\
&=& \displaystyle \sum_{j=0}^{k-1} r_{t-j} \\
\end{array} \]
</p>
</div>
</div>

<div id="outline-container-sec-2-5-3" class="outline-4">
<h4 id="sec-2-5-3"><span class="section-number-4">2.5.3</span> Dividends</h4>
<div class="outline-text-4" id="text-2-5-3">
<p>
Dividends are the payments received when companies distribution earnings to shareholders. Usually this is expressed as a <i>dividend yield</i>, which is quoted as a percentage of the market price of the share.
</p>

<p>
Since these payments are made when holding the asset (share), we need to incorporate them into our returns. Say we receive a dividend between time \(t-1\) and \(t\), we denote this \(D_t\) and our gross return becomes
</p>

<p>
\[ 1 + R_t = \frac{P_t + D_t}{P_{t-1}} = \frac{P_t}{P_{t-1}} + \frac{D_t}{P_{t-1}} \]
</p>
</div>
</div>

<div id="outline-container-sec-2-5-4" class="outline-4">
<h4 id="sec-2-5-4"><span class="section-number-4">2.5.4</span> Interconnectedness of indicies</h4>
<div class="outline-text-4" id="text-2-5-4">
<p>
An index is a aggregation of the value of a number of stocks in a particular market. They give a general idea of the performance of the market as whole, averaging out the performance of any particular constituent.
</p>

<p>
Typically they are constructed in one of two ways:
</p>
<ol class="org-ol">
<li>Price-weighted indices
The total capital invested into each share is proportional to the price of the share
</li>
<li>Value-weight indices
The total capital invested into each share is proportional to the total market capitalisation (value of outstanding equity)
</li>
</ol>

<p>
Often we will see that global indices are highly correlated. The top six indices of interest in this class are:
</p>
<ol class="org-ol">
<li>SP500
</li>
<li>Dow Jones (DJ)
</li>
<li>Han Seng (HSI)
</li>
<li>Nikkei 225
</li>
<li>Deutsche Aktien (DAX)
</li>
<li>FTSE
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-5-5" class="outline-4">
<h4 id="sec-2-5-5"><span class="section-number-4">2.5.5</span> Questions</h4>
<div class="outline-text-4" id="text-2-5-5">
<blockquote>
<p>
What is Jensen's inequality?
</p>
</blockquote>

<p>
Jensen's inequality provides a useful relationship between the expected value of some random variable \(X\) and some convex function \(g(x)\). An example proof can be found <a href="https://www.youtube.com/watch?v=10xgmpG_uTs">here</a>.
</p>

<p>
\[ \mathbb{E}[g(x)] \geq g(\mathbb{E}[x]) \text{, for a convex function } g \]
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-6" class="outline-3">
<h3 id="sec-2-6"><span class="section-number-3">2.6</span> <span class="label label-primary TODO">TODO</span> Lecture 6 - Risk aversion and simple models of returns</h3>
<div class="outline-text-3" id="text-2-6">
</div>
<div id="outline-container-sec-2-6-1" class="outline-4">
<h4 id="sec-2-6-1"><span class="section-number-4">2.6.1</span> Measuring co-movement of assets</h4>
<div class="outline-text-4" id="text-2-6-1">
<p>
Generally one aspect to be considered when choosing a portfolio of assets is the amount of co-movement. If all of our assets tend to move in the same direction then that is great when times are good but it also means that downfalls tend to be exaggerated. Depending on our appetite for risk we may instead seek assets that tend to move against each other in sharp down turns.
</p>

<p>
The typical method for estimating how assets move together is <i>covariance</i> or the standardised version <i>correlation</i>. These sample version of these quantities can be estimated by
</p>

<p>
\[ \widehat{\text{Cov}}(X_1, X_2) = \frac{1}{n} \sum_{t=1}^n [(X_{1t} - \bar{X}_1)(X_{2t} - \bar{X}_2)] \]
</p>

<p>
\[ \widehat{\text{Corr}}(X_1, X_2) = \frac{\widehat{\text{Cov}}(X_1, X_2)}{s_1 s_2} \]
</p>

<p>
where \(s_1\) and \(s_2\) are the sample variances for \(X_1\) and \(X_2\) respectively.
</p>
</div>
</div>

<div id="outline-container-sec-2-6-2" class="outline-4">
<h4 id="sec-2-6-2"><span class="section-number-4">2.6.2</span> Risk aversion</h4>
<div class="outline-text-4" id="text-2-6-2">
<p>
Risk aversion is the behavior of individuals who when exposed to uncertainty, attempt to lower that uncertainty. This implies generally that investors must bear more risk in order to earn greater returns. We tend to see this in historical asset returns. Assets with higher average returns in the long run tend also to exhibit greater volatility. Investors will only ever invest in an asset if they believe than over their time horizon they are more likely to realise a positive return than a negative return.
</p>

<p>
N.B. it is never guaranteed that investors will actually realise a greater return when taking on more risk, only that there is a possibility of a higher rate of return compared to a less risky asset.
</p>

<p>
This then summarises the risk-return trade-off. There is a positive relationship between risk and expected the return of an asset. Further, it is extremely difficult to outperform even random selections of stocks (Fama, 1965).
</p>
</div>
</div>

<div id="outline-container-sec-2-6-3" class="outline-4">
<h4 id="sec-2-6-3"><span class="section-number-4">2.6.3</span> Normality of net and log returns</h4>
<div class="outline-text-4" id="text-2-6-3">
<p>
We introduce two very naive models of returns.
</p>

<p>
For the first model we assume that net returns are normally distributed.
</p>

<p>
\[ R_t \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2) \]
</p>

<p>
However there are a few problems with this model:
</p>
<ul class="org-ul">
<li>Negative returns in reality are bounded by the value of the asset (usually)
</li>
<li>This doesn't tend to match the empirical return distributions for many instruments
</li>
</ul>

<p>
For the second model we assume that log returns are normally distributed.
</p>

<p>
\[ r_t = \log(1 + R_t) \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2) \]
</p>

<p>
A few more notes about this model:
</p>
<ul class="org-ul">
<li>This tends to be closer to empirical return distributions
</li>
</ul>

<p>
Using the second model, this then implies that the sum of \(k\) returns is also normally distributed (since this is simply the sum of normals)
</p>

<p>
\[ \log(1 + R_t(k)) = \mathcal{N}(k \mu, k \sigma^2) \]
</p>

<p>
By simple manipulations of random variables we can find the probabilities of being below a threshold in the following fashion
</p>

<p>
\[ \mathbb{P}(\log(1+R_t(k)) < x) = \mathbb{P} \left ( Z < \frac{\log(x) - k \mu}{\sqrt{k \sigma^2}} \right ) \]
</p>

<p>
with \(Z \sim \mathcal{N}(0,1)\).
</p>
</div>
</div>

<div id="outline-container-sec-2-6-4" class="outline-4">
<h4 id="sec-2-6-4"><span class="section-number-4">2.6.4</span> <span class="label label-primary TODO">TODO</span> Geometric random walk</h4>
<div class="outline-text-4" id="text-2-6-4">
<ul class="org-ul">
<li>Future returns are independent of the past, there is no way to predict the future
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-6-5" class="outline-4">
<h4 id="sec-2-6-5"><span class="section-number-4">2.6.5</span> <span class="label label-primary TODO">TODO</span> Questions</h4>
<div class="outline-text-4" id="text-2-6-5">
<blockquote>
<p>
What types of relationships does correlation measure?
</p>
</blockquote>

<p>
Correlation measures the strength of <i>linear relationships</i> between random variables. It is possible that non-linear relationships are not found when looking at simple correlation.
</p>

<blockquote>
<p>
Download the time series for IBM and BHP.
</p>
<ol class="org-ol">
<li>Calculate the sample correlation of the prices
</li>
<li>Calculate the sample correlation of the returns
</li>
<li>Calculate the sample correlation of the squared returns
</li>
</ol>

<p>
What do you notice?
</p>
</blockquote>

<blockquote>
<p>
Suppose the \(\text{log}(1 + R_t) \sim \mathcal{N}(\mu, \sigma^2)\). How does the mean, variance, skewness and kurtosis change for
</p>
<ol class="org-ol">
<li>1 period return
</li>
<li>5 period returns
</li>
<li>25 period returns
</li>
<li>100 period returns
</li>
</ol>
</blockquote>

<blockquote>
<p>
What is a qq-plot?
</p>
</blockquote>

<p>
A qq-plot compares the similarity of two distributions by plotting their quantiles against each other. They are very often used to compare a candidate distribution against a normal distribution.
</p>

<div class="org-src-container">

<pre class="src src-R">normal_draws <span style="color: #73d216;">&lt;-</span> rnorm<span style="color: #729fcf;">(</span><span style="color: #73d216;">1000</span><span style="color: #729fcf;">)</span>
qqnorm<span style="color: #729fcf;">(</span>normal_draws<span style="color: #729fcf;">)</span>
</pre>
</div>


<figure>
<p><img src="images/lec6_qq1.png" class="img-responsive" alt="lec6_qq1.png">
</p>
</figure>

<p>
Since these are indeed draws from a normal distribution we see the quantiles are strongly correlated. Let's compare it with a distribution with fatter tails, a t-distribution with 3 degrees of freedom.
</p>

<div class="org-src-container">

<pre class="src src-R">t_draws <span style="color: #73d216;">&lt;-</span> rt<span style="color: #729fcf;">(</span><span style="color: #73d216;">1000</span>, df=<span style="color: #73d216;">3</span><span style="color: #729fcf;">)</span>
qqnorm<span style="color: #729fcf;">(</span>t_draws<span style="color: #729fcf;">)</span>
</pre>
</div>


<figure>
<p><img src="images/lec6_qq2.png" class="img-responsive" alt="lec6_qq2.png">
</p>
</figure>
</div>
</div>
</div>

<div id="outline-container-sec-2-7" class="outline-3">
<h3 id="sec-2-7"><span class="section-number-3">2.7</span> <span class="label label-primary PROGRESS">PROGRESS</span> Lecture 7 - Forming sensible portfolios</h3>
<div class="outline-text-3" id="text-2-7">
</div>
<div id="outline-container-sec-2-7-1" class="outline-4">
<h4 id="sec-2-7-1"><span class="section-number-4">2.7.1</span> Portfolio with one risky asset</h4>
<div class="outline-text-4" id="text-2-7-1">
<p>
Given a risky asset \(R\) and a risk-free asset \(R_f\), a portfolio with weight \(w\) on the risky asset has a return of
</p>

<p>
\[ R_p = w R + (1-w) R_f \]
</p>

<p>
Given this, we can calculate the first two moments of the portfolio returns also
</p>

\begin{array}{rcl}
\mathbb{E}[R_p] &=& w \mu + (1-w) \mu_f \\
\mathbb{V}(R_p) &=& w^2 \sigma^2 \\
\end{array}

<p>
Note that we have no covariance term since the variance of \(R_f\) is zero (i.e. it is risk-free). We can therefore choose a suitable \(w\) based on the desires of the investor. We can pin this one of two ways.
</p>

<ol class="org-ol">
<li>Specify the desired rate of return
</li>
<li>Specify the desired level of risk
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-7-2" class="outline-4">
<h4 id="sec-2-7-2"><span class="section-number-4">2.7.2</span> Portfolio with two risky assets</h4>
<div class="outline-text-4" id="text-2-7-2">
<p>
Suppose instead that we have two risky assets to form a portfolio from, \(R_1\) and \(R_2\). Again we assign a weight of \(w\) to the first asset. The returns on the portfolio are
</p>

<p>
\[ R_p = w R_1 + (1-w) R_2 \]
</p>

<p>
Given this, we can calculate the first two moments of the portfolio returns also
</p>

\begin{array}{rcl}
\mathbb{E}[R_p] &=& w \mu_1 + (1-w) \mu_2 \\
\mathbb{V}(R_p) &=& w^2 \sigma^2_1 + (1-w)^2 \sigma^2_2 + 2 w (1-w) \text{Cov}(R_1, R_2) \\
\end{array}

<p>
Now things get a bit more involved since we have two variance terms and a covariance term to deal with. Suppose we wish to find the portfolio (defined by \(w\)) that minimises risk.
</p>

<p>
\[ \underset{w \in [0,1]}{\text{min}} \sigma^2_{R_p} \]
</p>

<p>
We can solve this by taking the derivative of the variance of the portfolio returns with respect to \(w\).
</p>

<p>
\[ \begin{array}{rcl}
\displaystyle \frac{d \sigma^2_{R_p}}{dw} &=& 2w\sigma^2_1 - 2(1-w) \sigma^2_2 + 2(1-w)\sigma_{12} - 2w\sigma_{12} = 0 \\
&=& \cdots \\
\therefore \hat{w} &=& \displaystyle \frac{\sigma^2_2 - \sigma_{12}}{\sigma^2_1 + \sigma^2_2 - 2\sigma_{12}} \\
\end{array} \]
</p>

<p>
where \(\sigma_{12} = \text{Cov}(R_1, R_2)\). This can be verified by taking the second derivative to ensure that the stationary point is a minimum.
</p>
</div>
</div>

<div id="outline-container-sec-2-7-3" class="outline-4">
<h4 id="sec-2-7-3"><span class="section-number-4">2.7.3</span> Connection with OLS</h4>
<div class="outline-text-4" id="text-2-7-3">
<p>
Let \(X = R_2 - R_1\) and \(Y = R_2\). Then
</p>

<p>
\[ \begin{array}{rcl}
\text{Cov}(Y, X) &=& \sigma^2_2 - \sigma_{12} \\
\mathbb{V}(X) &=& \sigma^2_1 + \sigma^2_2 - 2\sigma_{12} \\
\end{array} \]
</p>

<p>
Therefore we see that the optimal allocation seen previously can be found using OLS, where the weight \(w\) is the coefficient on \(X\) (the "covariate").
</p>
</div>
</div>

<div id="outline-container-sec-2-7-4" class="outline-4">
<h4 id="sec-2-7-4"><span class="section-number-4">2.7.4</span> <span class="label label-primary PROGRESS">PROGRESS</span> Questions</h4>
<div class="outline-text-4" id="text-2-7-4">
<blockquote>
<p>
What is the equity premium puzzle?
</p>
</blockquote>

<p>
Stocks have tended to outperform government bonds in excess of the increase in risk from holding stocks. This pattern has been observed in numerous markets, and has no solid explanation at the present. More information can be found <a href="https://www.investopedia.com/terms/e/epp.asp">here</a>.
</p>

<blockquote>
<p>
What is the capital market line? How does it relate to the Sharpe ratio and the efficient frontier?
</p>
</blockquote>

<p>
The capital market line gives the expected return for a given level of risk when optimally investing in the market portfolio and a risk-free asset.
</p>

<p>
\[ \mu_{R_p} = \mathbb{E}[R_p] = \mu_f + (\mu_R - \mu_f) \frac{\sigma_{R_p}}{\sigma_R} \]
</p>

<p>
As \(\mu_R > \mu_f\) for any reasonable investment, we see that we expect returns to increases linearly as we take on more risk (\(\sigma_{R_p}\)).
</p>

<blockquote>
<p>
What is lagrangian optimisation portfolio?
</p>
</blockquote>
</div>
</div>
</div>

<div id="outline-container-sec-2-8" class="outline-3">
<h3 id="sec-2-8"><span class="section-number-3">2.8</span> Lecture 8 - CAPM</h3>
<div class="outline-text-3" id="text-2-8">
</div><div id="outline-container-sec-2-8-1" class="outline-4">
<h4 id="sec-2-8-1"><span class="section-number-4">2.8.1</span> Rationale of CAPM</h4>
<div class="outline-text-4" id="text-2-8-1">
<p>
While the CML relates the market portfolio and a risk-free asset, telling you the expected return for a specific level of risk, CAPM builds upon this idea but expresses the risk relative to the rest of the market. Specifically it relates the excess returns on an asset to the excess returns of the market. One reason that we may want to phrase the portfolio construction in this way is since investing in the market already provides some level of returns (as seen in the CML), therefore we wish to see performance in excess of the market in order to invest in an individual asset as opposed to an index. Risk is now measured in terms of the correlation with the market returns.
</p>

<p>
We can fit model this as a linear regression equation
</p>

<p>
\[ R_{it} - R_{ft} = \alpha + \beta (R_{mt} - R_{ft}) + v_t \]
</p>

<p>
This equation is sometimes referred to as the security characteristic line. We typically use government bonds for \(R_{ft}\) and returns on a market index for \(R_{mt}\).
</p>
</div>
</div>

<div id="outline-container-sec-2-8-2" class="outline-4">
<h4 id="sec-2-8-2"><span class="section-number-4">2.8.2</span> \(\beta\) and \(\alpha\) interpretation</h4>
<div class="outline-text-4" id="text-2-8-2">
<p>
\(\beta\) gives a level of the risk relative to the market. For \(\beta = 1\), the covariance between the excess returns of the asset and the excess returns on the market are equal. That is to say that the asset is roughly as risky as investing in the overall market. For \(\beta > 1\) the asset is more risky than investing in the market portfolio and for \(0 < \beta < 1\) the asset moves with the market but less erratically. It is possible for \(\beta < 0\) when an asset is negatively correlated with the market. This is common for hedge portfolios (such as inverse ETFs).
</p>

<p>
\(\alpha\) on the other hand is a measure of the expected excess returns <b>in excess</b> of what is expected for the level of risk specified by \(\beta\). If \(\alpha > 0\) then this is evidence that we are getting more "bang for our buck" than we would have investing just in the market portfolio.
</p>
</div>
</div>

<div id="outline-container-sec-2-8-3" class="outline-4">
<h4 id="sec-2-8-3"><span class="section-number-4">2.8.3</span> Risk interpretation</h4>
<div class="outline-text-4" id="text-2-8-3">
<p>
Is \(\varepsilon_t\) (idiosyncratic risk) uncorrelated with systematic risk? This is the property of endogeneity and we require it in order to cleanly decompose risk into systematic risk (explained by market factors) and idiosyncratic risk. For CAPM models the \(R^2\) is a general measure of the systematic risk in the portfolio. Portfolios that show low \(R^2\) in CAPM fits are better hedges against the market as a whole since they contain less systematic risk.
</p>

<p>
See the "Diagnostics on disturbance term" in Lecture 10 for more information on this decomposition.
</p>
</div>
</div>

<div id="outline-container-sec-2-8-4" class="outline-4">
<h4 id="sec-2-8-4"><span class="section-number-4">2.8.4</span> Questions</h4>
<div class="outline-text-4" id="text-2-8-4">
<blockquote>
<p>
What is the relationship between a CAPM fit and \(R^2\)?
</p>
</blockquote>

<p>
\(R^2\) measures the level of portfolio risk due to systematic risk and \(1-R^2\) measures the idiosyncratic risk.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-9" class="outline-3">
<h3 id="sec-2-9"><span class="section-number-3">2.9</span> <span class="label label-primary PROGRESS">PROGRESS</span> Lecture 9 - CAPM fit</h3>
<div class="outline-text-3" id="text-2-9">
</div>
<div id="outline-container-sec-2-9-1" class="outline-4">
<h4 id="sec-2-9-1"><span class="section-number-4">2.9.1</span> Fama-French 3 factor model</h4>
<div class="outline-text-4" id="text-2-9-1">
<p>
In addition to the market risk factor (i.e. the broad performance of the market) there are at least two other factors that consistently explain return series or in other words "soak up" systematic risk in the model. These two factors are
</p>

<ul class="org-ul">
<li>SMB (small minus big) (market capitalisation)
<ul class="org-ul">
<li>This stems from the general observation that small stocks tend to outperform large stocks
</li>
<li>A coefficient of 0.5 or greater generally indicates the portfolio generates abnormal returns due to holding more small caps
</li>
</ul>
</li>
<li>HML (high minus low) (book to market ratio)
<ul class="org-ul">
<li>High <a href="https://www.investopedia.com/terms/b/booktomarketratio.asp">book to market</a> firms (value stocks) generally outperform lower book to market firms (growth stocks)
</li>
<li>A coefficient of 0.3 or greater generally indicates the portfolio generates abnormal returns due to holding value stocks
</li>
</ul>
</li>
</ul>

<p>
We are not concerned with the exact interpretations in this unit, but we are interested in the behaviour of this model since these market factors are random variables themselves.
</p>

<p>
In the manufacturing portfolio:
</p>
<ul class="org-ul">
<li>94% of variability is explained by market factors
</li>
<li>This indicates that we are mainly taking on systematic risk
</li>
<li>This means that we cannot use this portfolio as a hedge against the market, but idiosyncratic risk is hedged mostly (only 6% variability left)
</li>
</ul>

<p>
Technology portfolio:
</p>
<ul class="org-ul">
<li>Only 87% of variability explained by market factors
</li>
<li>More idiosyncratic risk that is not accounted for by market factors
</li>
<li>Positive alpha
</li>
<li>Beta is greater than 1, more volatile than market
</li>
<li>Almost entirely small cap
</li>
<li>Extreme growth stocks
</li>
<li>Possibly a better hedge against systemic risk
</li>
</ul>

<p>
This generally shows it is very hard to build a portfolio that is not explained mostly by market factors and therefore could be a hedge against systematic risk.
</p>
</div>
</div>

<div id="outline-container-sec-2-9-2" class="outline-4">
<h4 id="sec-2-9-2"><span class="section-number-4">2.9.2</span> Momentum factor</h4>
<div class="outline-text-4" id="text-2-9-2">
<p>
A potential four risk factor that consistently explains returns is <i>momentum</i>. Momentum "captures returns constructed by buying stocks with high returns and selling stocks with low returns over the same period". This was suggested in (Carhart, 1997) and claims to capture the hedging behaviour of investors (?).
</p>
</div>
</div>

<div id="outline-container-sec-2-9-3" class="outline-4">
<h4 id="sec-2-9-3"><span class="section-number-4">2.9.3</span> Multi-factor CAPM assumptions</h4>
<div class="outline-text-4" id="text-2-9-3">
<p>
Using all of these factors, we can construct a more comprehensive CAPM model
</p>

<p>
\[ R_{it} - R_{ft} = \alpha + \beta_1 (R_{mt} - R_{ft}) + \beta_2 \text{SMB}_t + \beta_3 \text{HML}_t + \beta_4 \text{MOM}_t + \varepsilon_t \]
</p>

<p>
We can determine the significance of the new factors by testing the joint significance of the new coefficients.
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \beta_2 = \beta_3 = \beta_4 = 0 \text{, single factor CAPM is sufficient} \\
H_1 &:& \text{At least one of } \beta_2, \beta_3, \beta_4 \text{ is non-zero, the single factor CAPM is insufficient} \\
\end{array} \]
</p>

<p>
We require that Gauss-Markov assumptions hold since this is just multivariate linear regression. Depending on the frequency of returns, the homoskedasticity assumption may hold, but tends to break down for higher frequency sampling. That is for the residuals we expect to see
</p>

<ul class="org-ul">
<li>Zero mean
</li>
<li>Constant variance
</li>
<li>No autocorrelation
</li>
<li>No endogeneity (i.e. \(\mathbb{E}[\varepsilon_t (R_{mt}-R_{ft})] = 0\) for example).
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-9-4" class="outline-4">
<h4 id="sec-2-9-4"><span class="section-number-4">2.9.4</span> Determining the quality of fit</h4>
<div class="outline-text-4" id="text-2-9-4">
<p>
When we estimate a CAPM model, \(R^2\) ends up being a measure of the systematic risk and \(1- R^2\) the idiosyncratic risk. We can also use hypothesis tests on the coefficients of market factors to make inference on what contributes to the returns of the portfolio. We typically see that the market risk factor explains about 70% of the systematic risk and the first three market factors typically explain 90%.
</p>
</div>
</div>

<div id="outline-container-sec-2-9-5" class="outline-4">
<h4 id="sec-2-9-5"><span class="section-number-4">2.9.5</span> <span class="label label-primary PROGRESS">PROGRESS</span> Questions</h4>
<div class="outline-text-4" id="text-2-9-5">
<blockquote>
<p>
What are the <i>spherical error</i> assumptions in the Gauss-Markov theorem?
</p>
</blockquote>

<ul class="org-ul">
<li>Zero mean
</li>
<li>Constant variance
</li>
<li>No autocorrelation
</li>
<li>No endogeneity
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-2-10" class="outline-3">
<h3 id="sec-2-10"><span class="section-number-3">2.10</span> <span class="label label-primary PROGRESS">PROGRESS</span> Lecture 10 - Model diagnostics</h3>
<div class="outline-text-3" id="text-2-10">
</div><div id="outline-container-sec-2-10-1" class="outline-4">
<h4 id="sec-2-10-1"><span class="section-number-4">2.10.1</span> Single parameter tests on CAPM</h4>
<div class="outline-text-4" id="text-2-10-1">
<blockquote>
<p>
Is the market factor significant? (\(\beta \not= 0\)?)
</p>
</blockquote>

<p>
Use T-test for small samples, but normal is fine for larger samples.
</p>

<blockquote>
<p>
Does a stock track the market?
</p>
</blockquote>

<p>
Test whether \(\beta=1\)
</p>

<blockquote>
<p>
Does a stock provide excess returns over or under the risk held?
</p>
</blockquote>

<p>
Test whether \(\alpha=0\)
</p>
</div>
</div>

<div id="outline-container-sec-2-10-2" class="outline-4">
<h4 id="sec-2-10-2"><span class="section-number-4">2.10.2</span> Multiple parameter tests on CAPM</h4>
<div class="outline-text-4" id="text-2-10-2">
<blockquote>
<p>
Does the 3-factor model improve upon standard CAPM?
</p>
</blockquote>

<p>
We test \(H_0: \beta_2 = \beta_3 = 0\)
</p>

<p>
Under the null we have
</p>

<p>
\[ J = \frac{RSS_0 - RSS_1}{RSS_1 / (T - k -1)} \sim \chi^2_{\ell} \]
</p>

<p>
where \(\ell\) is the number of restrictions (\(\ell=2\) here). Typically \(J\) is much larger than the critical value from the \(\chi^2\) distribution
</p>
</div>
</div>

<div id="outline-container-sec-2-10-3" class="outline-4">
<h4 id="sec-2-10-3"><span class="section-number-4">2.10.3</span> Diagnostics on disturbance term</h4>
<div class="outline-text-4" id="text-2-10-3">
<p>
Note in CAPM we assume that
</p>

<p>
\[ \begin{array}{rcl}
\mathbb{E}[\varepsilon_t] &=& 0 \\
\mathbb{E}[\varepsilon_t^2] &=& \sigma^2 \\
\mathbb{E}[\varepsilon_t \varepsilon_{t-j}] &=& 0 \text{ for } i \not= j \\
\mathbb{E}[\varepsilon_t (r_{mt} - r_{ft})] &=& 0 \\
\end{array} \]
</p>

<p>
We can test whether these properties hold using <b><b>lagrange multiplier</b></b> (LM) test. If these conditions do not hold then our estimates of the coeffecients are biased. These restrictions are also known as no exogeneity or endogeneity. We require no endogeneity to decompose risk into systematic risk and idiosyncratic risk in the following way
</p>

<p>
\[ \mathbb{E}[(R_{it} - R_{ft})^2] = \alpha + \beta^2 \mathbb{E}[(R_{mt} - R_{ft})^2] + \mathbb{E}[\varepsilon_t^2] \]
</p>

<p>
The squares of the residuals represent a proxy for the variance of the error term ($&epsilon;<sup>2</sup>). We wish to test if we can explain these by the regressors (which would imply heteroskedasticity) i.e. that the variance of the error term is dependent on the regressors and is non-constant. <b><b>White's test</b></b> constructs an auxiliary regression model including the level, square and cross products of each regressor and a constant. We can use the LM test statistic
</p>

<p>
\[ LM = T R^2 \]
</p>

<p>
which follows a \(\chi^2_{\ell}\)
</p>

<p>
where \(\ell\) is the number of restrictions (i.e. since we test if all the regressors in the auxiliary regression are 0 the number of terms in the auxiliary regression)
</p>
</div>
</div>

<div id="outline-container-sec-2-10-4" class="outline-4">
<h4 id="sec-2-10-4"><span class="section-number-4">2.10.4</span> ARCH effects</h4>
<div class="outline-text-4" id="text-2-10-4">
<p>
<b><b>Autoregressive conditional heteroskedasticity</b></b> (ARCH)
</p>

<p>
\[ \varepsilon^2_t = \gamma_0 + \gamma_1 \varepsilon^2_{t-1} + \gamma_2 \varepsilon^2_{t-2} + \cdots + \gamma_p \varepsilon^2_{t-p} + v_t \]
</p>

<p>
We test for \(\gamma_1 = \gamma_2 = \cdots = \gamma_p = 0\). The LM test statistic here is \(LM = T R^2\) and has a \(\chi^2_p\) distribution under the null
</p>

<ul class="org-ul">
<li>This is essentially testing for volatility clustering
</li>
<li>We will often find that the residuals of CAPM exhibit ARCH
</li>
<li>Therefore CAPM by itself does not tend to explain this behaviour
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-10-5" class="outline-4">
<h4 id="sec-2-10-5"><span class="section-number-4">2.10.5</span> <span class="label label-primary TODO">TODO</span> Questions</h4>
<div class="outline-text-4" id="text-2-10-5">
<blockquote>
<p>
What is exogeneity?
</p>
</blockquote>

<blockquote>
<p>
What is endogeneity?
</p>
</blockquote>
</div>
</div>
</div>

<div id="outline-container-sec-2-11" class="outline-3">
<h3 id="sec-2-11"><span class="section-number-3">2.11</span> Lecture 11 - Fixing OLS and EMH</h3>
<div class="outline-text-3" id="text-2-11">
</div>
<div id="outline-container-sec-2-11-1" class="outline-4">
<h4 id="sec-2-11-1"><span class="section-number-4">2.11.1</span> HAC (Newey-West) standard errors</h4>
<div class="outline-text-4" id="text-2-11-1">
<p>
In cases where heteroskedasticity or autocorrelation is present in the residual series the standard OLS estimators become inconsistent. We can use an alternative estimator for the covariance matric outlined in (Newey &amp; West, 1987) to obtain robust estimates for the standard errors. The point estimates however, do not need to be adjusted.
</p>

<p>
Using this process ensures that we are able to make valid interpretations of the estimates for coefficients in CAPM and therefore about the behaviour of a particular return series.
</p>
</div>
</div>

<div id="outline-container-sec-2-11-2" class="outline-4">
<h4 id="sec-2-11-2"><span class="section-number-4">2.11.2</span> Endogeneity</h4>
<div class="outline-text-4" id="text-2-11-2">
<p>
The final condition for CAPM to be valid is for the market risk factors to be uncorrelated with the disturbance term (which we interpret as the idiosyncratic of the return series). This can be expressed mathematically as
</p>

<p>
\[ \mathbb{E}[\varepsilon_t x_t] = \text{Cov}(\varepsilon_t, x_t) = 0 \]
</p>

<p>
If \(x_t\) is <i>endogenous</i>, then the OLS coefficient estimators are biased and inconsistent. Unlike serial correlation or heteroskedasticity, this means that the point estimates of the coefficients are also broken. We can test for this assumption but in ETC3460 we generally assume that it is not violated. In the case that it is violated we can use instrumental variable estimation in place of OLS. This work by Lars Hansen contributed to the 2013 Nobel memorial prize in economics.
</p>
</div>
</div>

<div id="outline-container-sec-2-11-3" class="outline-4">
<h4 id="sec-2-11-3"><span class="section-number-4">2.11.3</span> Efficient market hypothesis</h4>
<div class="outline-text-4" id="text-2-11-3">
<p>
The main motivation for investors participating in markets is to earn a profit. A key question therefore is whether future returns can be predicted. The <i>efficient market hypothesis</i> (EMH) states that this is a hopeless task. Specifically, if true the EMH implies that
</p>

<ol class="org-ol">
<li>The current price of an asset reflects all public and private information (this varies depending on the form of the EMH)
</li>
<li>The current price of an asset is not-informative for future returns
</li>
<li>Future returns are completely stochastic conditional on all previous information
</li>
<li>Investors cannot systematically use new information to turn a profit (even insiders)
</li>
</ol>

<p>
EMH is typically stated as weak, semi-strong or strong form. The differences between them depend on the claim of what set of information the prices reflect. For weak form the prices reflect the information in all previous prices. For semi-strong the prices reflect the information in past public information and finally the strong form encompasses all past information whether it be public or private. It is therefore implied by the strong form EMH that even executives cannot profit after they make key decisions and before public announcement.
</p>

<p>
Some further reading can be found here <a href="http://www.turtletrader.com/efficient-market.pdf">"Turtle trader" EMH</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-12" class="outline-3">
<h3 id="sec-2-12"><span class="section-number-3">2.12</span> Lecture 12 - Stationary series</h3>
<div class="outline-text-3" id="text-2-12">
</div>
<div id="outline-container-sec-2-12-1" class="outline-4">
<h4 id="sec-2-12-1"><span class="section-number-4">2.12.1</span> Predictability of returns vs prices</h4>
<div class="outline-text-4" id="text-2-12-1">
<p>
We should note that one of the key differences between returns and prices is their predictability. Take the model of asset prices following a geometric random walk. The prices are the value of the process and the returns are the random shocks. The value of the process itself is extremely predictable, but the returns are completely random.
</p>

<p>
Conditioning on past information of prices improves the predictions of future prices dramatically, but knowing last periods return in a random walk model provides no benefit whatsoever for forecasting future returns since they are independent random variables.
</p>
</div>
</div>

<div id="outline-container-sec-2-12-2" class="outline-4">
<h4 id="sec-2-12-2"><span class="section-number-4">2.12.2</span> Stationary and non-stationary series</h4>
<div class="outline-text-4" id="text-2-12-2">
<p>
A process is called <i>covariance stationary</i> or <i>weakly stationary</i> if and only if
</p>

<ul class="org-ul">
<li>\(\mathbb{E}[y_t] = \mu\), the process has a constant mean
</li>
<li>\(\mathbb{V}(y_t) = \sigma^2 < \infty\), the process has a constant finite variance
</li>
<li>\(\text{Cov}(y_t, y_{t-j}) = \gamma_j\), the covariance is dependent on the lag \(j\) and not the time \(t\)
</li>
</ul>

<p>
Further, a process is called <i>white noise</i> if and only if
</p>

<ul class="org-ul">
<li>\(\mathbb{E}[y_t] = 0\), the process has a constant mean of zero
</li>
<li>\(\mathbb{V}(y_t) = \sigma^2 < \infty\), the process has a constant finite variance
</li>
<li>\(\text{Cov}(y_t, y_{t-j}) = 0\) for \(j > 1\), the covariance between any two lags is zero
</li>
</ul>

<p>
In practice we measure inter-temporal dependence using the sample <i>autocorrelation function</i>.
</p>

<p>
\[ \hat{\gamma}(j) = \frac{ \sum_{t=k+1}^T  (r_t - \bar{r}) (r_{t-j} - \bar{r})}{ \sum_{t=1}^T (r_t - \bar{r})^2} \]
</p>

<p>
The corresponding population parameter is given (unsurprisingly) by
</p>

<p>
\[ \gamma(j) = \frac{ \text{Cov}(y_t, y_{t-j})}{ \mathbb{V}(y_t)} \]
</p>

<p>
If \(\{ y_t \}\) is a stationary process then \(\hat{\gamma}(j)\) asymptotically normal, so we can test the significance of the covariance terms (and whether or not they violate the EMH). The EMH implies that the residual series of a constant mean model fit to a return series should be white noise. Therefore we have a null hypothesis that \(\gamma_j  = 0\).
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \gamma_j = 0 \\
H_1 &:& \gamma_j \neq 0 \\
\end{array} \]
</p>

<p>
Since \(\hat{\gamma}_j \sim \mathcal{N}(0, \frac{1}{T})\) under \(H_0\) we use the test statistic \(\hat{\gamma_j} \times \sqrt{T}\) and reject when this is greater than 1.96 for a 5% significance level.
</p>
</div>
</div>

<div id="outline-container-sec-2-12-3" class="outline-4">
<h4 id="sec-2-12-3"><span class="section-number-4">2.12.3</span> AR resdiual model</h4>
<div class="outline-text-4" id="text-2-12-3">
<p>
We can use an LM test with the following form to determine if lags of the returns are informative for future returns. Assuming we have fit a constant mean model, on the residuals we fit
</p>

<p>
\[ \hat{\varepsilon}_t = \gamma_0 + \gamma_1 \hat{\varepsilon}_{t-1} + \gamma_2 \hat{\varepsilon}_{t-2} + \cdots + \gamma_k \hat{\varepsilon}_{t-k} + v_t \]
</p>

<p>
to test for autocorrelation up to order \(k\). \(v_t\) is some random disturbance term. We have the following hypotheses
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \gamma_1 = \gamma_2 = \cdots = \gamma_k \text{, no autocorrelation and EMH is satisfied} \\
H_1 &:& \text{At least one of } \gamma_1, \gamma_2, \ldots, \gamma_k \text{ is non-zero, autocorrelation present and EMH is not satisfied} \\
\end{array} \]
</p>

<p>
Under \(H_0\) the test statistic \(TR^2\) has a \(\chi^2_k\) distribution.
</p>
</div>
</div>

<div id="outline-container-sec-2-12-4" class="outline-4">
<h4 id="sec-2-12-4"><span class="section-number-4">2.12.4</span> Variance ratio test</h4>
<div class="outline-text-4" id="text-2-12-4">
<p>
If the returns in a series are indeed independent, then we know that the variance of a series increases proportional to the length of the sequence assuming that returns are normal. We can test for a deviation from this using a variance ratio test. We calculate the variance ratio as
</p>

<p>
\[ VR_n = \frac{s^2_n}{n \times s^2_1} \]
</p>

<p>
If the sequence has positive autocorrelation then we expect \(VR_n > 1\) since returns move in the same direction. If there is negative autocorrelation then we expect \(VR_n < 1\).
</p>

<p>
\[ VR_n = \begin{cases}
=1 & \text{No autocorrelation} \\
>1 & \text{Positive autocorrelation} \\
<1 & \text{Negative autocorrelation} \\
\end{cases} \]
</p>

<p>
Here we say that \(s^2_k\) is the sample estimator of the k period return. That is
</p>

<p>
\[ s^2_k = \frac{1}{T} \sum_{t=k+1}^T (r_k(t) - \bar{r}_k) \]
</p>
</div>
</div>

<div id="outline-container-sec-2-12-5" class="outline-4">
<h4 id="sec-2-12-5"><span class="section-number-4">2.12.5</span> Notes on the predictability of functions of the returns</h4>
<div class="outline-text-4" id="text-2-12-5">
<p>
While the level return series is generally unpredictable, the square (or higher moments) often are. The EMH only applies to the <i>level return series</i>. Therefore the predictability of volatility is not evidence against the EMH. Autocorrelation on the level return series is just one facet of the returns we can predict.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-13" class="outline-3">
<h3 id="sec-2-13"><span class="section-number-3">2.13</span> Lecture 13 - AR models</h3>
<div class="outline-text-3" id="text-2-13">
</div>
<div id="outline-container-sec-2-13-1" class="outline-4">
<h4 id="sec-2-13-1"><span class="section-number-4">2.13.1</span> AR(1) model</h4>
<div class="outline-text-4" id="text-2-13-1">
<p>
The motivation for even proposing more complicated models on the mean of returns is due to the poor evidence in favour of the EMH. Since it cannot be universally accepted across different assets and time periods, it is suggested that we may be able to model some aspects of the first moment of returns.
</p>

<p>
The simplest of these models is to assume that the current return is some linear combination of previous returns. For the AR(1) model, we take the case of just the previous return. Therefore our model for \(r_t\) is as follows
</p>

<p>
\[ r_t = \phi_0 + \phi_1 r_{t-1} + \varepsilon_t \]
</p>

<p>
where \(\varepsilon_t \sim \text{WN}(0, \sigma^2)\). Further, an AR(1) process is covariance stationary iff \(|\phi_1| < 1\).
</p>

<p>
The some of the key moments are outlined below
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="right">

<col  class="left">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-right">Moment</th>
<th scope="col" class="text-left">Condition</th>
<th scope="col" class="text-left">Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-right">1</td>
<td class="text-left">Unconditional</td>
<td class="text-left">\(\displaystyle \frac{\phi_0}{1 - \phi_1}\)</td>
</tr>

<tr>
<td class="text-right">2</td>
<td class="text-left">Unconditional</td>
<td class="text-left">\(\displaystyle \frac{\sigma^2}{1 - \phi_1^2}\)</td>
</tr>

<tr>
<td class="text-right">1</td>
<td class="text-left">Conditional on \(\mathcal{F}_{t-1}\)</td>
<td class="text-left">\(\phi_0 + \phi_1 r_{t-1}\)</td>
</tr>

<tr>
<td class="text-right">2</td>
<td class="text-left">Conditional on \(\mathcal{F}_{t-1}\)</td>
<td class="text-left">\(\sigma^2\)</td>
</tr>
</tbody>
</table>

<blockquote>
<p>
Exercise: Derive the unconditional variance using the law of iterated expectations
</p>
</blockquote>
</div>
</div>

<div id="outline-container-sec-2-13-2" class="outline-4">
<h4 id="sec-2-13-2"><span class="section-number-4">2.13.2</span> Forecasting using AR(p) models</h4>
<div class="outline-text-4" id="text-2-13-2">
<p>
To compute a one-step ahead forecast we simply use the observed value of the process at time \(t\) and the sample estimates of \(\gamma_0\) and \(\gamma_1\). That is, we calculate the estimated conditional mean as follows
</p>

<p>
\[ \hat{\mathbb{E}}[r_{t+1} | \mathcal{F}_{t-1}] = \hat{\gamma}_0 + \hat{\gamma}_1 r_{t} \]
</p>

<p>
For further forecasts (say to forecasts \(k\) periods ahead) we compute the forecast recursively, calculating one-step ahead from our \(k-1\) period ahead forecast. Therefore
</p>

<p>
\[ \begin{array}{rcl}
\hat{\mathbb{E}}[r_{t+k} | \mathcal{F}_{t-1}] &=& \hat{\gamma}_0 + \hat{\gamma}_1 \hat{\mathbb{E}}[r_{t+k-1}] \\
&=& \hat{\gamma}_0 (1 + \hat{\gamma}_1 + \hat{\gamma}_1^2 + \cdots + \hat{\gamma}_1^{k-1}) + \hat{\gamma}_1^k y_t \\
\end{array} \]
</p>

<p>
We note that the information in \(\mathcal{F}_{t-1}\) eventually becomes useless when the number of steps ahead we forecast grows large as the forecast approaches the long-run mean of the process. We can see this since
</p>

<p>
\[ \lim_{h \to \infty} 1 + \hat{\gamma}_1 + \hat{\gamma}_1^2 + \cdots + \hat{\gamma}_1^h = \sum_{h=1}^{\infty} \hat{\gamma}_1^h = \frac{1}{1 - \hat{\gamma}_1} \]
</p>

<p>
and also the second term approaches zero for any covariance stationary AR process (i.e. \(\lim_{h \to \infty} \hat{\gamma}_1^h y_t  = 0\) since \(|\hat{\gamma}_1| < 1\)).
</p>

<p>
For higher order AR models we simply repeat the recursive calculation but using the relevant formula and number of lags.
</p>
</div>
</div>

<div id="outline-container-sec-2-13-3" class="outline-4">
<h4 id="sec-2-13-3"><span class="section-number-4">2.13.3</span> Notes on selection of AR(p) models</h4>
<div class="outline-text-4" id="text-2-13-3">
<ul class="org-ul">
<li>Forecasts are typically consistent for well-specified AR models
</li>
<li>Use a sufficient \(p\) such that we have no serial correlation left in the residuals
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-13-4" class="outline-4">
<h4 id="sec-2-13-4"><span class="section-number-4">2.13.4</span> Non-parametric forecasting</h4>
<div class="outline-text-4" id="text-2-13-4">
<p>
For a more advanced method of forecasting the future return distribution, we can use the empirical distribution of returns rather than the assumption of a normal. Following this method, we would estimate the conditional mean and variance but then calculate confidence intervals or probability statements using past returns rather than a standard normal. More details on this can be found on slide 20 of the Without EMH slides.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-14" class="outline-3">
<h3 id="sec-2-14"><span class="section-number-3">2.14</span> Lecture 14 - Modelling volatility</h3>
<div class="outline-text-3" id="text-2-14">
</div>
<div id="outline-container-sec-2-14-1" class="outline-4">
<h4 id="sec-2-14-1"><span class="section-number-4">2.14.1</span> Why model volatility?</h4>
<div class="outline-text-4" id="text-2-14-1">
<p>
We have seen mixed evidence for EMH, indicating that we may be able to model the dynamics of the average return in a series, but the associated distribution of returns is significantly broader. This means that understanding the standard deviation of returns (and therefore the volatility or risk of the asset) can be very beneficial for an investor. We look at the standard deviation more often than the variance since it is in the same units as the returns themselves.
</p>

<p>
Volatility should be considered in conjunction with the time period it is measured. As variance is naturally proportional to time, volatility is proportional to root time. For example if \(\sigma\) is daily volatility, then the yearly volatility is \(\sqrt{252} \sigma\) (assuming 252 trading days a year). We call yearly volatility "annualized volatility" and rescasled to it to compare the volatility of different assets measured over different time frames. Note however that this naive scaling assumes no dependence between variance over time.
</p>
</div>
</div>

<div id="outline-container-sec-2-14-2" class="outline-4">
<h4 id="sec-2-14-2"><span class="section-number-4">2.14.2</span> Basic volatility models</h4>
<div class="outline-text-4" id="text-2-14-2">
<p>
We look at modelling the conditional variance using a known function \(g\) of \(\mathcal{F}_{t-1}\) the information up until time \(t-1\).
</p>

<p>
\[ \mathbb{V}(\varepsilon_t | \mathcal{F}_{t-1}) = g(\mathcal{F}_{t-1}) \]
</p>

<p>
We have already observed that volatility tends to cluster together (this is one of the stylized facts we identified in the first few lessons). This observation suggests that \(g\) may be genuinely informative to use for estimating future variance and risk.
</p>
</div>
</div>

<div id="outline-container-sec-2-14-3" class="outline-4">
<h4 id="sec-2-14-3"><span class="section-number-4">2.14.3</span> ARCH models</h4>
<div class="outline-text-4" id="text-2-14-3">
<p>
Suppose we specify the following for \(g\)
</p>

<p>
\[ g(\mathcal{F}_{t-1}) = \mathbb{V}(\varepsilon_t | \mathcal{F}_{t-1}) = \alpha_0 + \sum_{l=0}^p \alpha_l \varepsilon_{t-l}^2 \]
</p>

<p>
where we assume \(\alpha_i > 0\) for \(i \geq 1\) in a well-estimated model since we require variance to be a positive quantity. We also assume that \(\sum_{i=1}^p \alpha_i < 1\) for a well defined unconditional variance.
</p>

<p>
This model is known as the autoregressive conditional heteroskedasticity (ARCH) model. It has this name since we are resolving serial correlation in the squared return series by application of an AR model. This is an effective technique since we have established that variance of return series are time-varying and persistent.
</p>

<p>
An ARCH model is sufficient to capture three of the four stylized facts we went over in the first few lessons, namely:
</p>
<ul class="org-ul">
<li>Heavy tails in return distributions
</li>
<li>No persistence in the level of returns
</li>
<li>Volatility clustering
</li>
</ul>

<p>
However, it fails to model the negative skewness that is often apparent in return series.
</p>

<p>
Often we specify the form of ARCH models in the following way.
</p>

<p>
\[ \begin{array}{rcl}
r_t &=& \mathbb{E}[r_t | \mathcal{F}_{t-1}] + \varepsilon_t \\
\varepsilon_t &=& \sigma_t u_t \\
u_t &\sim& \mathcal{N}(0, 1) \\
\sigma_t^2 &=& \alpha_0 + \alpha_1 u_{t-1}^2 + \cdots + \alpha_p u_{t-p}^2 \\
\end{array} \]
</p>

<p>
Often we are not interested in the raw residuals \(\varepsilon_t\) but in the <i>standardised</i> residuals i.e. \(\varepsilon_t / \sigma_t\).
</p>
</div>
</div>

<div id="outline-container-sec-2-14-4" class="outline-4">
<h4 id="sec-2-14-4"><span class="section-number-4">2.14.4</span> <span class="label label-primary TODO">TODO</span> Questions</h4>
<div class="outline-text-4" id="text-2-14-4">
<blockquote>
<p>
What is a mixture model?
</p>
</blockquote>

<blockquote>
<p>
What are the first two unconditional moments of a \(ARCH(1)\) model?
</p>
</blockquote>
</div>
</div>
</div>

<div id="outline-container-sec-2-15" class="outline-3">
<h3 id="sec-2-15"><span class="section-number-3">2.15</span> <span class="label label-primary TODO">TODO</span> Lecture 15 - ARCH moments</h3>
<div class="outline-text-3" id="text-2-15">
</div>
<div id="outline-container-sec-2-15-1" class="outline-4">
<h4 id="sec-2-15-1"><span class="section-number-4">2.15.1</span> <span class="label label-primary TODO">TODO</span> Analytic unconditional moments of ARCH(1)</h4>
</div>
<div id="outline-container-sec-2-15-2" class="outline-4">
<h4 id="sec-2-15-2"><span class="section-number-4">2.15.2</span> <span class="label label-primary TODO">TODO</span> News impact curve (NIC)</h4>
</div>
<div id="outline-container-sec-2-15-3" class="outline-4">
<h4 id="sec-2-15-3"><span class="section-number-4">2.15.3</span> <span class="label label-primary TODO">TODO</span> Questions</h4>
<div class="outline-text-4" id="text-2-15-3">
<blockquote>
<p>
Fit an ARCH(1) model to something very volatile and look at the \(\alpha_1\). Is it greater than \(\sqrt{\frac{1}{3}}\) ?
</p>
</blockquote>

<blockquote>
<p>
How much persistence do we see in real market volatility? Would this be expected under an ARCH(1) model?
</p>
</blockquote>

<ul class="org-ul">
<li>AR(1) on the squared returns, low persistence
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-2-16" class="outline-3">
<h3 id="sec-2-16"><span class="section-number-3">2.16</span> <span class="label label-primary TODO">TODO</span> Lecture 16 - GARCH models</h3>
<div class="outline-text-3" id="text-2-16">
</div>
<div id="outline-container-sec-2-16-1" class="outline-4">
<h4 id="sec-2-16-1"><span class="section-number-4">2.16.1</span> <span class="label label-primary TODO">TODO</span> Forecasts using AR(1) with ARCH(1) errors</h4>
</div>
<div id="outline-container-sec-2-16-2" class="outline-4">
<h4 id="sec-2-16-2"><span class="section-number-4">2.16.2</span> <span class="label label-primary TODO">TODO</span> Diagnostics for ARCH models</h4>
<div class="outline-text-4" id="text-2-16-2">
<ul class="org-ul">
<li>We can look at the autocorrelation in the remaining residuals
</li>
<li>We can retest for ARCH effects (have we used a high enough order of ARCH?)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-16-3" class="outline-4">
<h4 id="sec-2-16-3"><span class="section-number-4">2.16.3</span> <span class="label label-primary TODO">TODO</span> Different distributions for \(u_t\)</h4>
<div class="outline-text-4" id="text-2-16-3">
<ul class="org-ul">
<li>Often we make the assumption that conditionally our errors are scaled normals
</li>
<li>We can relax this and look at other error distributions like Student's T or Generalized error distribution
</li>
<li>We re-estimate the model by maximising a different likelihood function
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-16-4" class="outline-4">
<h4 id="sec-2-16-4"><span class="section-number-4">2.16.4</span> <span class="label label-primary TODO">TODO</span> GARCH models</h4>
<div class="outline-text-4" id="text-2-16-4">
<ul class="org-ul">
<li>ARCH models struggle to model autocorrelation in the squared residuals
</li>
<li>GARCH includes lags of \(\varepsilon^2\) and \(\sigma^2\)
</li>
<li>This captures more of the autoregressive behaviour we see in volatility
</li>
</ul>


<figure>
<p><img src="./images/lec16_arch_vol.png" class="img-responsive" alt="lec16_arch_vol.png" width="500px">
</p>
</figure>


<figure>
<p><img src="./images/lec16_garch_vol.png" class="img-responsive" alt="lec16_garch_vol.png" width="500px">
</p>
</figure>
</div>
</div>

<div id="outline-container-sec-2-16-5" class="outline-4">
<h4 id="sec-2-16-5"><span class="section-number-4">2.16.5</span> <span class="label label-primary TODO">TODO</span> GARCH models as a ARCH in squares</h4>
<div class="outline-text-4" id="text-2-16-5">
<ul class="org-ul">
<li>We can rewrite this as a ARMA model
</li>
<li>How can we get requirements for stationarity from this form?
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-2-17" class="outline-3">
<h3 id="sec-2-17"><span class="section-number-3">2.17</span> Lecture 17 - GARCH properties</h3>
<div class="outline-text-3" id="text-2-17">
</div><div id="outline-container-sec-2-17-1" class="outline-4">
<h4 id="sec-2-17-1"><span class="section-number-4">2.17.1</span> GARCH predictions</h4>
<div class="outline-text-4" id="text-2-17-1">
<p>
Suppose we have estimated an AR(1) GARCH(1,1) model. How do we generate predictions from this?
</p>

<p>
To start, we can do a one-step ahead point estimate of the returns quite easily. In fact we only need to look at the mean model. Since GARCH models are just ARMA models in the squares, we can calculate forecasts in much the same way as the old mean models we have already looked at.
</p>

<p>
I've done some of these on paper, including looking at the long-run variance estimates (which approach the unconditional variance).
</p>
</div>
</div>

<div id="outline-container-sec-2-17-2" class="outline-4">
<h4 id="sec-2-17-2"><span class="section-number-4">2.17.2</span> Comparison between ARCH and GARCH predictions</h4>
<div class="outline-text-4" id="text-2-17-2">
<p>
Empirically we tend to find that GARCH models produce tighter intervals around forecasts of future returns. We can see an example of this on Slide 91 where the following intervals are produced for a one step ahead forecast on IBM monthly returns.
</p>

<p>
\[ \begin{array}{rcl}
\text{ARCH} &:& [-0.1292, 0.1437] \\
\text{GARCH} &:& [-0.0975, 0.1097] \\
\end{array} \]
</p>

<p>
In addition to the width of the intervals, we should also consider the density that each fitted model places on the realized return when evaluating them. For instance we can look at the forecasted probability that the return would be less than the realized return.
</p>


<figure>
<p><img src="./images/tut17_arch_garch_prob.png" class="img-responsive" alt="tut17_arch_garch_prob.png" width="500px">
</p>
</figure>

<p>
We can visualise the forecasted distributions like so.
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>tidyverse<span style="color: #729fcf;">)</span>
<span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>reshape2<span style="color: #729fcf;">)</span>

tibble<span style="color: #729fcf;">(</span>x = seq<span style="color: #8ae234;">(</span>-<span style="color: #73d216;">0.3</span>, <span style="color: #73d216;">0.3</span>, length.out = <span style="color: #73d216;">100</span><span style="color: #8ae234;">)</span>,
       arch = dnorm<span style="color: #8ae234;">(</span>x, mean=<span style="color: #73d216;">0.0071</span>, sd=<span style="color: #73d216;">0.0695</span><span style="color: #8ae234;">)</span>,
       garch = dnorm<span style="color: #8ae234;">(</span>x, mean=<span style="color: #73d216;">0.0061</span>, sd=<span style="color: #73d216;">0.0529</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  melt<span style="color: #729fcf;">(</span>id = c<span style="color: #8ae234;">(</span><span style="color: #de7fa8;">"x"</span><span style="color: #8ae234;">)</span>, value.name = <span style="color: #de7fa8;">"density"</span>, variable.name = <span style="color: #de7fa8;">"model"</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  ggplot<span style="color: #729fcf;">()</span> +
  geom_line<span style="color: #729fcf;">(</span>aes<span style="color: #8ae234;">(</span>x=x, y=density, col=model<span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> +
  geom_vline<span style="color: #729fcf;">(</span>aes<span style="color: #8ae234;">(</span>xintercept=<span style="color: #73d216;">0.0562</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> +
  ggtitle<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"ARCH vs GARCH forecast distributions"</span>,
          subtitle=<span style="color: #de7fa8;">"IBM monthly returns for Feb 2018"</span><span style="color: #729fcf;">)</span>
</pre>
</div>


<figure>
<p><img src="images/lec17_forecast_dist.png" class="img-responsive" alt="lec17_forecast_dist.png">
</p>
</figure>
</div>
</div>

<div id="outline-container-sec-2-17-3" class="outline-4">
<h4 id="sec-2-17-3"><span class="section-number-4">2.17.3</span> Likelihood ratio tests</h4>
<div class="outline-text-4" id="text-2-17-3">
<p>
As a GARCH(1,1) nests a ARCH(1) model (i.e. it is an extension of an ARCH(1)), we can use a likelihood ratio test in order to determine the statistical significance of the new parameters.
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \text{New parameters are insignificant} \\
H_1 &:& \text{New parameters are significant} \\
\end{array} \]
</p>

<p>
This is completed by:
</p>
<ol class="org-ol">
<li>Calculating the likelihood \(LL(\hat{\theta}_G)\) of the GARCH model
</li>
<li>Calculating the likelihood \(LL(\hat{\theta}_A)\) of the ARCH model
</li>
<li>Calculating the test statistic \(LR = 2 (LL(\hat{\theta}_G) - LL(\hat{\theta}_A))\)
</li>
<li>Calculating the critical value \(C\) from a \(\chi^2_q\) model where \(q\) is the number of additional parameters
</li>
<li>Reject the null when \(LR > C\)
</li>
</ol>

<p>
This testing process generalizes to other types of models also, not just ARCH / GARCH.
</p>
</div>
</div>

<div id="outline-container-sec-2-17-4" class="outline-4">
<h4 id="sec-2-17-4"><span class="section-number-4">2.17.4</span> Testing for asymmetric volatility</h4>
<div class="outline-text-4" id="text-2-17-4">
<p>
Empirically we tend to observe stronger reactions to negative shocks in most assets than we do to positive shocks. This so called <i>asymmetric volatility</i> is a characteristisation of one of the stylzied facts we have seen (i.e. the negative tails in return distributions). There are many explanations for why this might occur, for instance the leverage effect or consumption smoothing.
</p>

<p>
We can test whether this may need to be accounted for in a particular return series by using LM tests on the residual series to see if we can explain them using features related to the direction of past shocks. Some examples are below
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Test name</th>
<th scope="col" class="text-left">Auxiliary regresion</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">General asymmetry test</td>
<td class="text-left">\(\hat{u}_t^2 = \phi_0 + \phi_1 \mathbb{I}(\hat{\varepsilon}_{t-1} < 0) + v_t\)</td>
</tr>

<tr>
<td class="text-left">Negative size test</td>
<td class="text-left">\(\hat{u}_t^2 = \phi_0 + \phi_1 \mathbb{I}(\hat{\varepsilon}_{t-1} < 0) \hat{\varepsilon}_{t-1} + v_t\)</td>
</tr>

<tr>
<td class="text-left">Positive size test</td>
<td class="text-left">\(\hat{u}_t^2 = \phi_0 + \phi_1 \mathbb{I}(\hat{\varepsilon}_{t-1} > 0) \hat{\varepsilon}_{t-1} + v_t\)</td>
</tr>

<tr>
<td class="text-left">Everything test</td>
<td class="text-left">\(\hat{u}_t^2 = \phi_0 + \phi_1 \mathbb{I}(\hat{\varepsilon}_{t-1} < 0) + \phi_2 \mathbb{I}(\hat{\varepsilon}_{t-1} < 0) \hat{\varepsilon}_{t-1} + \phi_3 \mathbb{I}(\hat{\varepsilon}_{t-1} > 0) \hat{\varepsilon}_{t-1} + v_t\)</td>
</tr>
</tbody>
</table>

<p>
For each of these, the test statistic is \(\chi^2_p\) where \(p\) is the number of regressors under \(H_0\) that the regressors are insignificant.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-18" class="outline-3">
<h3 id="sec-2-18"><span class="section-number-3">2.18</span> Lecture 18 - Asymmetric volatility</h3>
<div class="outline-text-3" id="text-2-18">
</div>
<div id="outline-container-sec-2-18-1" class="outline-4">
<h4 id="sec-2-18-1"><span class="section-number-4">2.18.1</span> Asymmetric news impacts</h4>
<div class="outline-text-4" id="text-2-18-1">
<p>
LM test with auxiliary regression equation
</p>

<p>
\[ \hat{u}_t^2 = \phi_0 + \phi_1 I_{t-1}^- + \eta_t \]
</p>

<p>
or
</p>

<p>
\[ \hat{u}_t^2 = \phi_0 + \phi_1 I_{t-1}^+ + \eta_t \]
</p>

<p>
\(TR^2 \sim \chi_1^2\) under \(H_0: \phi_1 = 0\).
</p>
</div>
</div>

<div id="outline-container-sec-2-18-2" class="outline-4">
<h4 id="sec-2-18-2"><span class="section-number-4">2.18.2</span> Size effect tests</h4>
<div class="outline-text-4" id="text-2-18-2">
<ul class="org-ul">
<li>$ \hat{u}<sub>t</sub><sup>2</sup> = &phi;<sub>0</sub> + &phi;<sub>1</sub> I<sub>t-1</sub>^- \hat{\varepsilon}<sub>t-1</sub> + &eta;<sub>t</sub> $ (negative size test)
<ul class="org-ul">
<li>If \(\phi_1 < 0\) then negative shocks raise volatility by more than positive shocks
</li>
</ul>
</li>
<li>$ \hat{u}<sub>t</sub><sup>2</sup> = &phi;<sub>0</sub> + &phi;<sub>1</sub> I<sub>t-1</sub>^+ \hat{\varepsilon}<sub>t-1</sub> + &eta;<sub>t</sub> $ (positive size test)
<ul class="org-ul">
<li>If \(\phi_1 < 0\) then positive shocks raise volatility by more than positive shocks
</li>
</ul>
</li>
</ul>

<p>
You may also test for a more complete auxiliary equation including the direction and size
</p>

<p>
\[ \hat{u}_t^2 = \phi_0 + \phi_1 I_{t-1}^- + \phi_2 I_{t-1}^+ \hat{\varepsilon}_{t-1} + \phi_3 I_{t-1}^- \hat{\varepsilon}_{t-1}  + \eta_t \]
</p>
</div>
</div>

<div id="outline-container-sec-2-18-3" class="outline-4">
<h4 id="sec-2-18-3"><span class="section-number-4">2.18.3</span> GJR-GARCH model</h4>
<div class="outline-text-4" id="text-2-18-3">
<p>
To model the skew in responses to negative shocks, we can add regressors into our variance model to allow for modelling of this behaviour. In a GJR-GARCH(p, o, q) model we add regressors \(o\) terms that are of the form \(\lambda_j \mathbb{I}(\hat{\varepsilon}_t < 0) \hat{\varepsilon}_t^2\). In ETC3460 we generally look at \(o=1\). The full model now looks like this for a GJR-GARCH(1,1,1) model (with constant mean).
</p>

<p>
\[ \begin{array}{rcl}
r_t &=& \mu + \varepsilon_t \\
\varepsilon_t &=& \sigma_t u_t \\
\sigma^2_t &=& \alpha_0 + \alpha_1 \varepsilon_{t-1}^2 + \lambda (I_{t-1} \varepsilon_{t-1}^2) + \beta_1 \sigma_{t-1}^2 \\
I_{t-1} &=& 1 \text{ if } \varepsilon_{t-1} \leq 0 \text{ else } 0
\end{array} \]
</p>

<p>
Typically we observe \(\lambda > 0\) in stock markets.
</p>

<p>
Note this creates a "kink" around \(\varepsilon_{t-1} = 0\) on the NIC (since the new term is additive for negative shocks). Requires \(\alpha_0, \alpha_1, \beta_1 \geq 0\) and \(\lambda + \alpha_1 \geq 0\) to be valid (i.e. we require positive volatility).
</p>
</div>
</div>

<div id="outline-container-sec-2-18-4" class="outline-4">
<h4 id="sec-2-18-4"><span class="section-number-4">2.18.4</span> <span class="label label-primary PROGRESS">PROGRESS</span> TGARCH / TARCH model</h4>
<div class="outline-text-4" id="text-2-18-4">
<p>
Another way of expressing the magnitude of the move is using the absolute size of the move rather than the square.
</p>

<p>
\[ \begin{array}{rcl}
r_t &=& \mu + \varepsilon_t \\
\varepsilon_t &=& \sigma_t u_t \\
\sigma_t &=& \alpha_0 + \alpha_1 | \varepsilon_{t-1} | + \lambda (I_{t-1} | \varepsilon_{t-1} |) + \beta_1 \sigma_{t-1} \\
I_{t-1} &=& 1 \text{ if } \varepsilon_{t-1} \leq 0 \text{ else } 0
\end{array} \]
</p>

<p>
This is like GJR-GARCH but in the volatility not the squared volatility (i.e. using absolute value instead of squares). This helps capture more heavily tailed data (e.g. for when not even the second moment doesn't exist in the data) (why?).
</p>

<p>
Requires \(\alpha_0, \alpha_1, \beta_1 \geq 0\) and \(\lambda + \alpha_1 \geq 0\) to be valid (i.e. we require positive volatility).
</p>
</div>
</div>

<div id="outline-container-sec-2-18-5" class="outline-4">
<h4 id="sec-2-18-5"><span class="section-number-4">2.18.5</span> <span class="label label-primary PROGRESS">PROGRESS</span> EGARCH</h4>
<div class="outline-text-4" id="text-2-18-5">
<p>
An EGARCH model smoothes out the kink seen in GJR-GARCH and TARCH models.
</p>

<p>
\[ \begin{array}{rcl}
r_t &=& \mu + \varepsilon_t \\
\varepsilon_t &=& \sigma_t u_t \\
\ln \sigma^2_t &=& \alpha_0 + \alpha_1 ( | u_{t-1} | - \mathbb{E}[| u_{t-1} |]) + \gamma u_{t-1} + \beta_1 \ln \sigma^2_{t-1} \\
\end{array} \]
</p>

<p>
for \(u_t \overset{\text{i.i.d.}}{\sim} N(0,1)\), \(\mathbb{E}[| u_{t-1} |] = \sqrt{2/\pi}\). The difference between the absolute standardised shock and its expectation acts as a smoother around 0 shocks (whereas GJR-GARCH and TARCH models use an indicator function and therefore have a kink in the NIC around 0) (does it actually tho?).
</p>
</div>
</div>

<div id="outline-container-sec-2-18-6" class="outline-4">
<h4 id="sec-2-18-6"><span class="section-number-4">2.18.6</span> Notes on the significance and forecasting of asymmetric volatility models</h4>
<div class="outline-text-4" id="text-2-18-6">
<p>
We can use likelihood ratio tests with GJR-GARCH and TGARCH models against GARCH since they are nested models. However, since EGARCH models the log volatility we cannot use likelihood ratio tests since it doesn't nest a GARCH.
</p>

<p>
Since asymmetric models look at the sign of past shocks (or some interpolation based on the sign of the shock) we cannot compute closed form forecasts or long run variance easily. Typically we instead use simulation and monte-carlo estimates to estimate distributions. This is particularly difficult for EGARCH where we model the log volatility.
</p>
</div>
</div>

<div id="outline-container-sec-2-18-7" class="outline-4">
<h4 id="sec-2-18-7"><span class="section-number-4">2.18.7</span> Questions</h4>
<div class="outline-text-4" id="text-2-18-7">
<blockquote>
<p>
What is an alpha-stable distribution?
</p>
</blockquote>

<p>
Let \(X_1\) and \(X_2\) be i.i.d. RVs. \(X_1\) and \(X_2\) are said to be stable if and only if for any constants \(a, b, c > 0\) and \(d\) the linear combination \(aX_1 + bX_2\) is equal to \(cX +d\) for \(X\) that follows the same distribution as \(X_1\) and \(X_2\). We say that The distribution is strictly stable if \(d=0\). For example the normal distribution satisfies this property. Let \(X, X_1, X_2 \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2)\). Then
</p>

<p>
\[ aX_1 + bX_2 = \sqrt{a^2 + b^2} X_3 \]
</p>

<p>
Link to wikipedia page with more information <a href="https://en.wikipedia.org/wiki/Stable_distribution">here</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-19" class="outline-3">
<h3 id="sec-2-19"><span class="section-number-3">2.19</span> <span class="label label-primary TODO">TODO</span> Lecture 19 - Multivariate GARCH</h3>
<div class="outline-text-3" id="text-2-19">
</div>
<div id="outline-container-sec-2-19-1" class="outline-4">
<h4 id="sec-2-19-1"><span class="section-number-4">2.19.1</span> <span class="label label-primary TODO">TODO</span> Volatility interplay between assets</h4>
<div class="outline-text-4" id="text-2-19-1">
<ul class="org-ul">
<li>GARCH models look at the conditional volatility of a single asset
</li>
<li>However they are univariate, and therefore assume that the volatility in other assets is indepdent
</li>
<li>In practice this assumption breaks down
</li>

<li>Revisiting CAPM, this implies that \(\beta\) (i.e. the correlation between an asset and the market times the ratio in volatilities) is time-varying
</li>
<li>This ends up implying that for almost all return series we have time-varying variance, ARCH effects and non-constant covariance with the market
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-19-2" class="outline-4">
<h4 id="sec-2-19-2"><span class="section-number-4">2.19.2</span> <span class="label label-primary TODO">TODO</span> Rolling covariance</h4>
<div class="outline-text-4" id="text-2-19-2">
<div class="org-src-container">

<pre class="src src-R"><span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>tidyquant<span style="color: #729fcf;">)</span>
<span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>PerformanceAnalytics<span style="color: #729fcf;">)</span>
<span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>reshape2<span style="color: #729fcf;">)</span>

data <span style="color: #73d216;">&lt;-</span> tq_get<span style="color: #729fcf;">(</span>x = c<span style="color: #8ae234;">(</span><span style="color: #de7fa8;">"AAPL"</span>, <span style="color: #de7fa8;">"MSFT"</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
data <span style="color: #73d216;">%&gt;%</span>
  select<span style="color: #729fcf;">(</span>symbol, date, adjusted<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  dcast<span style="color: #729fcf;">(</span>date ~ symbol<span style="color: #729fcf;">)</span> <span style="color: #73d216;">-&gt;</span> data_wide
data_xts <span style="color: #73d216;">&lt;-</span> xts<span style="color: #729fcf;">(</span>data_wide<span style="color: #8ae234;">[</span>,-<span style="color: #73d216;">1</span><span style="color: #8ae234;">]</span>, data_wide$date<span style="color: #729fcf;">)</span>
returns <span style="color: #73d216;">&lt;-</span> CalculateReturns<span style="color: #729fcf;">(</span>data_xts<span style="color: #729fcf;">)</span>
chart.RollingCorrelation<span style="color: #729fcf;">(</span>returns$AAPL, returns$MSFT<span style="color: #729fcf;">)</span>
</pre>
</div>


<figure>
<p><img src="test.png" class="img-responsive" alt="test.png">
</p>
</figure>
</div>
</div>

<div id="outline-container-sec-2-19-3" class="outline-4">
<h4 id="sec-2-19-3"><span class="section-number-4">2.19.3</span> <span class="label label-primary TODO">TODO</span> Multivariate volatility modelling</h4>
</div>
<div id="outline-container-sec-2-19-4" class="outline-4">
<h4 id="sec-2-19-4"><span class="section-number-4">2.19.4</span> <span class="label label-primary TODO">TODO</span> Contagion and non-parametric methods</h4>
</div>
</div>
<div id="outline-container-sec-2-20" class="outline-3">
<h3 id="sec-2-20"><span class="section-number-3">2.20</span> <span class="label label-primary TODO">TODO</span> Lecture 20 -</h3>
<div class="outline-text-3" id="text-2-20">
</div>
</div>
<div id="outline-container-sec-2-21" class="outline-3">
<h3 id="sec-2-21"><span class="section-number-3">2.21</span> Lecture 21 - Review lecture</h3>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Laboratories</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Week 2 laboratory</h3>
<div class="outline-text-3" id="text-3-1">
</div>
<div id="outline-container-sec-3-1-1" class="outline-4">
<h4 id="sec-3-1-1"><span class="section-number-4">3.1.1</span> Overview</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
In this tutorial we went through the process of accessing EViews through MOVE and working through some exercises on a dataset of the prices of NASDAQ.
</p>

<p>
After opening the time series we could create a variable holding the log returns using
</p>

<div class="org-src-container">

<pre class="src src-EViews">log_ret = log(close/close(-1))
</pre>
</div>

<p>
and storing the first lag in another variable by
</p>

<div class="org-src-container">

<pre class="src src-EViews">log_ret_lag = log_ret(-1)
</pre>
</div>

<p>
We then regressed the log returns on the first lag and a constant to generate a simple model to forecast ahead log returns
</p>

<div class="org-src-container">

<pre class="src src-EViews">log_ret c log_ret_lag
</pre>
</div>


<figure>
<p><img src="./images/lab1_ar_1.png" class="img-responsive" alt="lab1_ar_1.png">
</p>
</figure>

<p>
Alternatively we could use the <code>AR(1)</code> shortcut to generate the same model
</p>

<div class="org-src-container">

<pre class="src src-EViews">log_ret c AR(1)
</pre>
</div>

<p>
We also generated a number of models that average over the previous \(k\) values of the series. These are known as movement average models or <code>MA(k)</code> for short. We can estimate them in EViews like so.
</p>

<pre class="example">
mov3 = @movav(log_ret_lag, 3)
</pre>

<p>
Popular choices may include MA(3), MA(5), MA(7) models however this is completely arbitrary. Lastly we went over the interpretation of some of the measures of fit for our forecats. The measures we covered in the lab included
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="left">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Measurement</th>
<th scope="col" class="text-left">Acronym</th>
<th scope="col" class="text-left">Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">Root mean squared error</td>
<td class="text-left">RMSE</td>
<td class="text-left">\(\displaystyle \sqrt{\frac{1}{T} \sum_{i=1}^T (y_i - \hat{y}_i)^2 }\)</td>
</tr>

<tr>
<td class="text-left">Mean absolute error</td>
<td class="text-left">MAE</td>
<td class="text-left">\(\displaystyle \frac{1}{T} \sum_{i=1}^T \mid y_i - \hat{y}_i \mid\)</td>
</tr>

<tr>
<td class="text-left">Mean absolute percentage error</td>
<td class="text-left">MAPE</td>
<td class="text-left">\(\displaystyle \frac{100}{T} \sum_{i=1}^T \frac{\mid y_i - \hat{y}_i \mid}{y_i}\)</td>
</tr>

<tr>
<td class="text-left">Mean absolute scaled error</td>
<td class="text-left">MASE</td>
<td class="text-left">\(\displaystyle \frac{1}{T} \sum_{i=1}^T \frac{y_i - \hat{y}_i}{\text{MAE}}\)</td>
</tr>

<tr>
<td class="text-left">Theil coefficient</td>
<td class="text-left">&#xa0;</td>
<td class="text-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
For more detailed information on most of these accuracy measures, the <a href="https://otexts.com/fpp2/accuracy.html">accuracy section in FPP2</a> is quite useful. Finally here we can see a forecast evaluation summary from EViews
</p>


<figure>
<p><img src="./images/lab1_ar_1_fit.png" class="img-responsive" alt="lab1_ar_1_fit.png">
</p>
</figure>
</div>
</div>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Week 3 laboratory</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="outline-container-sec-3-2-1" class="outline-4">
<h4 id="sec-3-2-1"><span class="section-number-4">3.2.1</span> Overview</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
In this lab we use EViews to calculate raw and standardised moments of four different distributions. Following this we use the third and fourth standardised moments in a simple hypothesis test to test for normality.
</p>
</div>
</div>

<div id="outline-container-sec-3-2-2" class="outline-4">
<h4 id="sec-3-2-2"><span class="section-number-4">3.2.2</span> Question one</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
Raw moments of a distribution can be calculated easily using <code>@mean</code> macro. For example to calculate the third raw moment of a series <code>x</code>, we can use the following snippet.
</p>

<div class="org-src-container">

<pre class="src src-EViews">scalar raw_moment_3 = @mean(@pow(x, 3))
</pre>
</div>

<p>
This is expected since the sample estimator for the third moment is simply
</p>

<p>
\[ \hat{\mu}_3 = \frac{1}{N} \sum_{i=1}^N x_i^3 \]
</p>

<p>
For standardised moments, we need to subtract by the mean and divide by the variance.
</p>

<p>
\[ \hat{\bar{\mu}}_3^s = \frac{1}{N} \sum_{i=1}^N \left ( \frac{x_i - \hat{\mu}_1}{\hat{\bar{\mu}}_2} \right ) ^3 \]
</p>

<p>
In EViews, this looks like
</p>

<div class="org-src-container">

<pre class="src src-EViews">scalar stan_moment_3 = @mean(@pow((x-@mean(x))/@var(x), 3))
</pre>
</div>

<p>
You can compare your answers with the descriptive statistics generated by EViews.
</p>
</div>
</div>

<div id="outline-container-sec-3-2-3" class="outline-4">
<h4 id="sec-3-2-3"><span class="section-number-4">3.2.3</span> Question two</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
In this question we use a dataset with draws from three different distributions and conduct two hypothesis tests. Firstly, we are interested in whether the distributions have 0 mean. By central limit theorem we have that
</p>

<p>
\[ \bar{X} \sim \mathcal{N} \left ( \mu_X, \frac{\sigma^2}{n} \right ) \]
</p>

<p>
and by simple manipulation we have
</p>

<p>
\[ \frac{\sqrt{n}(\bar{X} - \mu_X)}{\sigma} \sim \mathcal{N}(0,1) \]
</p>

<p>
We can use this form to test the set of hypotheses
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \mu_X = 0 \\
H_1 &:& \mu_X \not= 0 \\
\end{array} \]
</p>

<p>
However we don't know the true value of \(\sigma\), only the sample variance \(s^2\). Therefore we test using the distribution
</p>

<p>
\[ \text{T-statistic} = \frac{\sqrt{n}(\bar{X} - 0)}{s} \sim t(n-1) \text{, under } H_0 \]
</p>

<p>
Meaning that we reject when the T-statistic is outside the middle 95% of the distribution (or equivalent for other confidence levels).
</p>

<p>
The second test we use is Jarque-Bera test for noramlity. We have established that normal distributions are parameterised entirely by their mean and variance while their skewness and excess kurtosis are 0. We can therefore form a simple hypothesis test to determine if we have evidence to discredit this in the sample we have drawn.
</p>

<p>
The test statistic for Jarque-Bera is
</p>

<p>
\[ \text{JB} = \frac{n}{6} \left ( (\hat{\bar{\mu}}_3^s - 0)^2 + \frac{(\hat{\bar{\mu}}_4^s-3)^2}{4} \right ) \]
</p>

<p>
and follows a \(\chi_2^2\) distribution under the null hypothesis of normality. In our case we found all three distributions were likely to be non-normal.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> <span class="label label-primary PROGRESS">PROGRESS</span> Week 4 laboratory</h3>
<div class="outline-text-3" id="text-3-3">
</div><div id="outline-container-sec-3-3-1" class="outline-4">
<h4 id="sec-3-3-1"><span class="section-number-4">3.3.1</span> Question one</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
For the below, suppose \(r_t\) is the log returns for an asset and that \(r_t \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2)\).
</p>

<blockquote>
<p>
What is the distribution of the 2-period return \(r_t(2) = r_t + r_{t-1}\)
</p>
</blockquote>

<p>
This is the sum of two independent normals, therefore we have \(r_t(2) \sim \mathcal{N}(2\mu, 2\sigma^2)\)
</p>

<blockquote>
<p>
Let \(\mu = 0.1\) and \(\sigma^2 = 0.6\). Find \(\mathbb{P}(r_t(2) < 2)\)
</p>
</blockquote>

<p>
We can standardise the returns and reduce this to looking up the appropriate value in a standard normal distribution.
</p>

<p>
\[ \begin{array}{rcl}
\mathbb{P}(r_t(2) < 2) &=& \displaystyle \mathbb{P}(\frac{r_t(2) - 2\mu}{\sqrt{2 \sigma^2}} < \frac{2 - 2\mu}{\sqrt{2 \sigma^2}}) \\
&=& \displaystyle \mathbb{P}(\frac{r_t(2) - 0.2}{1.095} < \frac{2 - 0.2}{1.095}) \\
&=& \displaystyle \mathbb{P}(Z < 1.643) \\
\end{array} \]
</p>

<div class="org-src-container">

<pre class="src src-R">paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"The probability is"</span>, round<span style="color: #8ae234;">(</span>pnorm<span style="color: #fce94f;">(</span><span style="color: #73d216;">1.643</span><span style="color: #fce94f;">)</span>, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
</pre>
</div>

<pre class="example">
The probability is 0.95

</pre>

<blockquote>
<p>
What is the covariance between \(r_2(2)\) and \(r_5(2)\)?
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
r_2(2) &=& r_2 + r_1 \\
r_5(2) &=& r_5 + r_4 \\
\text{Cov}(r_2(2), r_5(2)) &=& \text{Cov}(r_2 + r_1, r_5 + r_4) \\
&=& \text{Cov}(r_2, r_5 + r_4) + \text{Cov}(r_1, r_5 + r_4) \\
&=& \text{Cov}(r_2, r_5) + \text{Cov}(r_2, r_4) + \text{Cov}(r_1, r_5) + \text{Cov}(r_1, r_4) \\
&=& 0 \\
\end{array} \]
</p>

<blockquote>
<p>
What is the distribution of \(r_t(2)\) given that \(r_{t-1} = 100\)?
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
r_t(2) &=& r_t + r_{t-1} \\
&=& r_t + 100 \\
&\sim& \mathcal{N}(\mu + 100, \sigma^2) \\
\end{array} \]
</p>
</div>
</div>

<div id="outline-container-sec-3-3-2" class="outline-4">
<h4 id="sec-3-3-2"><span class="section-number-4">3.3.2</span> Question two</h4>
<div class="outline-text-4" id="text-3-3-2">

<figure>
<p><img src="./images/tute4_2_1.png" class="img-responsive" alt="tute4_2_1.png" width="600px">
</p>
</figure>

<ul class="org-ul">
<li>The first standardised moment is always zero since we subtract the mean and don't raise it to any higher powers
</li>
<li>The second standardised moment is always one since we divide the second centralised moment by the variance (also the second centralised moment)
</li>
<li>The third standardised moment is skewness, in this case nearly zero
</li>
<li>The fourth standardised moment is kurtosis, in this case about 81, indicating much fatter tails than the normal
</li>
</ul>

<p>
The distribution is likely not normal, the Jarque-Bera statistic (which has a \(\chi^2_1\) under \(H_0\) of normality) is enourmous. The test whether the mean of the return series is non-zero, we use the test statistic of
</p>

<div class="org-src-container">

<pre class="src src-EVIEWS">scalar t_stat = @sqrt(2055)*@mean(log_ret)/@sqrt(@var(log_ret))
</pre>
</div>

<p>
which is t(2054) under the null, 1.329, giving a p-value of 0.184
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #73d216;">2</span>*<span style="color: #729fcf;">(</span><span style="color: #73d216;">1</span> - pt<span style="color: #8ae234;">(</span><span style="color: #73d216;">1.329</span>, df=<span style="color: #73d216;">2054</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
</pre>
</div>

<pre class="example">
0.183995576442415

</pre>

<p>
When we estimate the following model
</p>

<p>
\[ r_t = \mu + \rho r_{t-1} + \varepsilon_t \]
</p>

<p>
We obtain an estimate of \(\hat{\rho} \approx -0.244\). Using a T-test to determine the significance against zero we obtain a p-value of practically zero. Therefore we conclude that the average return is believed to be non-zero.
</p>
</div>
</div>

<div id="outline-container-sec-3-3-3" class="outline-4">
<h4 id="sec-3-3-3"><span class="section-number-4">3.3.3</span> Question three</h4>
<div class="outline-text-4" id="text-3-3-3">

<figure>
<p><img src="./images/tute4_3_1.png" class="img-responsive" alt="tute4_3_1.png" width="1000px">
</p>
</figure>

<p>
We now look at the returns for ANZ from 2008 to 2018. In the previous question we looked at the returns from 2000 to 2007. The kurtosis of the return distribution has dropped by a factor of 8. We now see slight positive skewness. We still reject the null hypothesis of normality by a wide margin. The dot com bubble was present across the first sample but since the 2008 equity volatility has been consistently low and markets have been quiet. This has led to fewer extreme events and lower kurtosis in the return series as a result compared to 2000 to 2007.
</p>
</div>
</div>

<div id="outline-container-sec-3-3-4" class="outline-4">
<h4 id="sec-3-3-4"><span class="section-number-4">3.3.4</span> <span class="label label-primary PROGRESS">PROGRESS</span> Question four</h4>
<div class="outline-text-4" id="text-3-3-4">
<p>
Done on paper, answers were also in group chat. Good exercise.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> Week 5 laboratory</h3>
<div class="outline-text-3" id="text-3-4">
</div><div id="outline-container-sec-3-4-1" class="outline-4">
<h4 id="sec-3-4-1"><span class="section-number-4">3.4.1</span> Question one</h4>
<div class="outline-text-4" id="text-3-4-1">
<p>
My new NBN internet connection has just died so I'm doing this tutorial in R. We first compute the monthly log returns for Microsoft and Walmart.
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>tidyverse<span style="color: #729fcf;">)</span>
<span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>lubridate<span style="color: #729fcf;">)</span>

rets <span style="color: #73d216;">&lt;-</span> read_csv<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"data/msft_wmt.csv"</span>, col_types=cols<span style="color: #8ae234;">()</span><span style="color: #729fcf;">)</span>
rets <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>msft_log_ret = log<span style="color: #8ae234;">(</span>MSFT<span style="color: #8ae234;">)</span> - log<span style="color: #8ae234;">(</span>lag<span style="color: #fce94f;">(</span>MSFT<span style="color: #fce94f;">)</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>wmt_log_ret = log<span style="color: #8ae234;">(</span>WMT<span style="color: #8ae234;">)</span> - log<span style="color: #8ae234;">(</span>lag<span style="color: #fce94f;">(</span>WMT<span style="color: #fce94f;">)</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>date = dmy<span style="color: #8ae234;">(</span>Date<span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  na.omit<span style="color: #729fcf;">()</span> <span style="color: #73d216;">%&gt;%</span>
  select<span style="color: #729fcf;">(</span>date, msft_log_ret, wmt_log_ret<span style="color: #729fcf;">)</span> <span style="color: #73d216;">-&gt;</span> rets

head<span style="color: #729fcf;">(</span>rets<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span> as.data.frame<span style="color: #729fcf;">()</span>
</pre>
</div>

<pre class="example">

        date msft_log_ret wmt_log_ret
1 1990-02-01   0.06538309 0.031748698
2 1990-03-01   0.11468355 0.071263020
3 1990-04-01   0.04631562 0.049042085
4 1990-05-01   0.23001599 0.127531059
5 1990-06-01   0.04027421 0.101138756
6 1990-07-01  -0.13353193 0.005994024

</pre>


<p>
We then compute the covariance matrix of the returns
</p>

<div class="org-src-container">

<pre class="src src-R">cov<span style="color: #729fcf;">(</span>as.matrix<span style="color: #8ae234;">(</span>rets<span style="color: #fce94f;">[</span>,c<span style="color: #de7fa8;">(</span><span style="color: #73d216;">2</span>,<span style="color: #73d216;">3</span><span style="color: #de7fa8;">)</span><span style="color: #fce94f;">]</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">-&gt;</span> cov_matrix
cov_matrix
</pre>
</div>

<pre class="example">

             msft_log_ret wmt_log_ret
msft_log_ret  0.009625086 0.001987965
wmt_log_ret   0.001987965 0.004770311

</pre>

<p>
From this we can compute the optimal weights for a two asset portfolio.
</p>

<div class="org-src-container">

<pre class="src src-R">w1 <span style="color: #73d216;">&lt;-</span> <span style="color: #729fcf;">(</span>cov_matrix<span style="color: #8ae234;">[</span><span style="color: #73d216;">1</span><span style="color: #8ae234;">]</span> - cov_matrix<span style="color: #8ae234;">[</span><span style="color: #73d216;">2</span><span style="color: #8ae234;">]</span><span style="color: #729fcf;">)</span>/<span style="color: #729fcf;">(</span>cov_matrix<span style="color: #8ae234;">[</span><span style="color: #73d216;">1</span><span style="color: #8ae234;">]</span> + cov_matrix<span style="color: #8ae234;">[</span><span style="color: #73d216;">4</span><span style="color: #8ae234;">]</span> - <span style="color: #73d216;">2</span>*cov_matrix<span style="color: #8ae234;">[</span><span style="color: #73d216;">2</span><span style="color: #8ae234;">]</span><span style="color: #729fcf;">)</span>
w2 <span style="color: #73d216;">&lt;-</span> <span style="color: #73d216;">1</span> - w1
paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"The optimal weights are "</span>, round<span style="color: #8ae234;">(</span><span style="color: #73d216;">100</span>*w1, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span>, <span style="color: #de7fa8;">"% on MSFT and "</span>, round<span style="color: #8ae234;">(</span><span style="color: #73d216;">100</span>*w2, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span>, <span style="color: #de7fa8;">"% on WMT"</span>, sep=<span style="color: #de7fa8;">""</span><span style="color: #729fcf;">)</span>
</pre>
</div>

<pre class="example">
The optimal weights are 73.3% on MSFT and 26.7% on WMT

</pre>

<p>
Alternatively we can come to these estimates using OLS rather than looking at the covariance matrix. We create the following series.
</p>

<p>
\[ \begin{array}{rcl}
X_t &=& \text{MSFT}_t - \text{WMT}_t \\
Y_t &=& \text{MSFT}_t \\
\end{array} \]
</p>

<p>
and solve the linear regression problem
</p>

<p>
\[ Y_t = \alpha + \beta X_t + \varepsilon_t \]
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>broom<span style="color: #729fcf;">)</span>

rets <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>X = msft_log_ret - wmt_log_ret<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>Y = msft_log_ret<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  select<span style="color: #729fcf;">(</span>X, Y<span style="color: #729fcf;">)</span> <span style="color: #73d216;">-&gt;</span> df

model <span style="color: #73d216;">&lt;-</span> lm<span style="color: #729fcf;">(</span>Y ~ X, data=df<span style="color: #729fcf;">)</span>
tidy<span style="color: #729fcf;">(</span>model<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span> as.data.frame<span style="color: #729fcf;">()</span>
</pre>
</div>

<pre class="example">

         term   estimate  std.error statistic      p.value
1 (Intercept) 0.01138367 0.00412199  2.761694 6.200737e-03
2           X 0.73296654 0.04038423 18.149822 9.808939e-47

</pre>

<p>
Here we see that the estimated coefficient \(\hat{\beta}_1 = w_1\). We can plot the residuals to look for any interesting patterns.
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #73d216;">library</span><span style="color: #729fcf;">(</span>ggplot2<span style="color: #729fcf;">)</span>

data.frame<span style="color: #729fcf;">(</span>date = rets$date,
           resid = model$residuals<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  ggplot<span style="color: #729fcf;">()</span> + geom_line<span style="color: #729fcf;">(</span>aes<span style="color: #8ae234;">(</span>x=date, y=resid<span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
</pre>
</div>


<figure>
<p><img src="images/week5_tute_resid.png" class="img-responsive" alt="week5_tute_resid.png">
</p>
</figure>

<p>
We can see some trending and time varying volatility. It's unlikely that these residuals satisfy the assumption of white noise. Either way, let's assume our estimates for the coefficients and associated standard errors are fine and conduct a hypothesis test about whether an equally weighted portfolio is suitable for these assets.
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \beta = \frac{1}{2} \\
H_1 &:& \beta \not= \frac{1}{2} \\
\end{array} \]
</p>

<p>
Our test statistic and critical value at the 1% level can be calculated in R.
</p>

<div class="org-src-container">

<pre class="src src-R">test_statistic <span style="color: #73d216;">&lt;-</span> <span style="color: #729fcf;">(</span>coef<span style="color: #8ae234;">(</span>model<span style="color: #8ae234;">)[</span><span style="color: #73d216;">2</span><span style="color: #8ae234;">]</span> - <span style="color: #73d216;">0.5</span><span style="color: #729fcf;">)</span> / tidy<span style="color: #729fcf;">(</span>model<span style="color: #729fcf;">)</span>$std.error<span style="color: #729fcf;">[</span><span style="color: #73d216;">2</span><span style="color: #729fcf;">]</span>
critical_value <span style="color: #73d216;">&lt;-</span> qt<span style="color: #729fcf;">(</span><span style="color: #73d216;">0.99</span>, df = nrow<span style="color: #8ae234;">(</span>rets<span style="color: #8ae234;">)</span>-<span style="color: #73d216;">1</span>-<span style="color: #73d216;">1</span><span style="color: #729fcf;">)</span>

paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"The test statistic is"</span>, round<span style="color: #8ae234;">(</span>test_statistic, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span>, <span style="color: #de7fa8;">"and the critical value is"</span>, round<span style="color: #8ae234;">(</span>critical_value, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
</pre>
</div>

<p>
[1] "The test statistic is 5.77 and the critical value is 2.34"
</p>

<p>
Therefore we reject the null hypothesis and conclude an equally weighted portfolio would be suitable.
</p>
</div>
</div>

<div id="outline-container-sec-3-4-2" class="outline-4">
<h4 id="sec-3-4-2"><span class="section-number-4">3.4.2</span> Question two</h4>
<div class="outline-text-4" id="text-3-4-2">
<p>
We will import some data on the risk free rate and excess returns on the market portfolio to combine with our returns for MSFT and WMT.
</p>

<div class="org-src-container">

<pre class="src src-R">rf_mktrf <span style="color: #73d216;">&lt;-</span> read_csv<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"data/msft_wmt_rf_mktrf.csv"</span>, col_types=cols<span style="color: #8ae234;">()</span><span style="color: #729fcf;">)</span>
rf_mktrf <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>date = dmy<span style="color: #8ae234;">(</span>Date<span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  rename<span style="color: #729fcf;">(</span>mkt_rf = <span style="color: #eeeeec; background-color: #262B2C;">`Mkt-RF`</span>, rf = RF<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>rf = rf/<span style="color: #73d216;">100</span>, mkt_rf = mkt_rf/<span style="color: #73d216;">100</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  select<span style="color: #729fcf;">(</span>date, mkt_rf, rf<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  left_join<span style="color: #729fcf;">(</span>rets, by=<span style="color: #de7fa8;">"date"</span><span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  na.omit<span style="color: #729fcf;">()</span> <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>msft_excess = msft_log_ret - rf<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  mutate<span style="color: #729fcf;">(</span>wmt_excess = wmt_log_ret - rf<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span>
  select<span style="color: #729fcf;">(</span>date, mkt_rf, msft_excess, wmt_excess<span style="color: #729fcf;">)</span> <span style="color: #73d216;">-&gt;</span> capm_df

head<span style="color: #729fcf;">(</span>capm_df<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span> as.data.frame<span style="color: #729fcf;">()</span>
</pre>
</div>

<pre class="example">

        date  mkt_rf msft_excess    wmt_excess
1 1990-02-01  0.0111  0.05968309  0.0260486983
2 1990-03-01  0.0183  0.10828355  0.0648630200
3 1990-04-01 -0.0336  0.03941562  0.0421420851
4 1990-05-01  0.0842  0.22321599  0.1207310588
5 1990-06-01 -0.0109  0.03397421  0.0948387562
6 1990-07-01 -0.0190 -0.14033193 -0.0008059761

</pre>

<p>
We will fit a single factor CAPM model to each of MSFT and WMT.
</p>

<div class="org-src-container">

<pre class="src src-R">msft_capm <span style="color: #73d216;">&lt;-</span> lm<span style="color: #729fcf;">(</span>msft_excess ~ <span style="color: #73d216;">1</span> + mkt_rf, data=capm_df<span style="color: #729fcf;">)</span>
wmt_capm <span style="color: #73d216;">&lt;-</span> lm<span style="color: #729fcf;">(</span>wmt_excess ~ <span style="color: #73d216;">1</span> + mkt_rf, data=capm_df<span style="color: #729fcf;">)</span>

tidy<span style="color: #729fcf;">(</span>msft_capm<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span> as.data.frame<span style="color: #729fcf;">()</span>
tidy<span style="color: #729fcf;">(</span>wmt_capm<span style="color: #729fcf;">)</span> <span style="color: #73d216;">%&gt;%</span> as.data.frame<span style="color: #729fcf;">()</span>
</pre>
</div>

<pre class="example">

         term   estimate   std.error statistic      p.value
1 (Intercept) 0.00673088 0.005216442   1.29032 1.981970e-01
2      mkt_rf 1.27547755 0.117330865  10.87078 1.348746e-22

         term    estimate  std.error statistic      p.value
1 (Intercept) 0.003450597 0.00411977 0.8375704 4.031159e-01
2      mkt_rf 0.622044074 0.09266397 6.7129014 1.396180e-10

</pre>

<p>
We can see that in both cases \(\alpha\) is insignificantly different from 0 at any reasonable significance level. We can also test whether \(\beta\) is likely to be different from 1.
</p>

<div class="org-src-container">

<pre class="src src-R">critical_value <span style="color: #73d216;">&lt;-</span> qt<span style="color: #729fcf;">(</span><span style="color: #73d216;">0.95</span>, df = nrow<span style="color: #8ae234;">(</span>capm_df<span style="color: #8ae234;">)</span> - <span style="color: #73d216;">2</span><span style="color: #729fcf;">)</span>

msft_test_statistic <span style="color: #73d216;">&lt;-</span> abs<span style="color: #729fcf;">(</span><span style="color: #8ae234;">(</span>tidy<span style="color: #fce94f;">(</span>msft_capm<span style="color: #fce94f;">)</span>$estimate<span style="color: #fce94f;">[</span><span style="color: #73d216;">2</span><span style="color: #fce94f;">]</span> - <span style="color: #73d216;">1</span><span style="color: #8ae234;">)</span> / tidy<span style="color: #8ae234;">(</span>msft_capm<span style="color: #8ae234;">)</span>$std.error<span style="color: #8ae234;">[</span><span style="color: #73d216;">2</span><span style="color: #8ae234;">]</span><span style="color: #729fcf;">)</span>
msft_tracks_market <span style="color: #73d216;">&lt;-</span> msft_test_statistic &lt; critical_value

paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"The test statistic is"</span>, round<span style="color: #8ae234;">(</span>msft_test_statistic, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span>, <span style="color: #de7fa8;">"and the critical value is"</span>, round<span style="color: #8ae234;">(</span>critical_value, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"Based on the evidence, does MSFT track the market?"</span>, msft_tracks_market<span style="color: #729fcf;">)</span>

wmt_test_statistic <span style="color: #73d216;">&lt;-</span> abs<span style="color: #729fcf;">(</span><span style="color: #8ae234;">(</span>tidy<span style="color: #fce94f;">(</span>wmt_capm<span style="color: #fce94f;">)</span>$estimate<span style="color: #fce94f;">[</span><span style="color: #73d216;">2</span><span style="color: #fce94f;">]</span> - <span style="color: #73d216;">1</span><span style="color: #8ae234;">)</span> / tidy<span style="color: #8ae234;">(</span>wmt_capm<span style="color: #8ae234;">)</span>$std.error<span style="color: #8ae234;">[</span><span style="color: #73d216;">2</span><span style="color: #8ae234;">]</span><span style="color: #729fcf;">)</span>
wmt_tracks_market <span style="color: #73d216;">&lt;-</span> wmt_test_statistic &lt; critical_value

paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"The test statistic is"</span>, round<span style="color: #8ae234;">(</span>wmt_test_statistic, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span>, <span style="color: #de7fa8;">"and the critical value is"</span>, round<span style="color: #8ae234;">(</span>critical_value, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"Based on the evidence, does WMT track the market?"</span>, wmt_tracks_market<span style="color: #729fcf;">)</span>
</pre>
</div>

<pre class="example">

[1] "The test statistic is 2.35 and the critical value is 1.65"

[1] "Based on the evidence, does MSFT track the market? FALSE"

[1] "The test statistic is 4.08 and the critical value is 1.65"

[1] "Based on the evidence, does WMT track the market? FALSE"

</pre>

<p>
We see evidence that both stocks have betas greater or less than 1. That is, there is evidence that neither stock tracks the market and is either more or less volatile.
</p>

<p>
We omit residual tests.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> <span class="label label-primary TODO">TODO</span> Week 6 laboratory</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Generate excess return series
</p>

<div class="org-src-container">

<pre class="src src-EViews">series other_excess = other - rf
</pre>
</div>

<p>
Estimate 3 factor model
</p>

<div class="org-src-container">

<pre class="src src-EViews">other_excess c mkt_rf smb hml momentum
</pre>
</div>


<figure>
<p><img src="./images/tut6_3fact.png" class="img-responsive" alt="tut6_3fact.png">
</p>
</figure>

<p>
we can test if \(\alpha=0\) with a t-test, t(n-k) t(1101)
</p>

<div class="org-src-container">

<pre class="src src-EViews">@qdist(0.975, 1101)
</pre>
</div>

<p>
we reject the null and conclude alpha is nonzero
</p>

<p>
any hypothesis tests about the coefficients rely on assumptions holding, lets check the residuals
</p>


<figure>
<p><img src="./images/tut6_3fact_residuals.png" class="img-responsive" alt="tut6_3fact_residuals.png">
</p>
</figure>

<p>
Looking at Jarque-Bera statistic, we reject the null hypothesis that the first four moments of the distribution are consistent with a normal.
</p>

<p>
we now test for heteroskedasticity using whites test
</p>


<figure>
<p><img src="./images/tut6_3fact_white.png" class="img-responsive" alt="tut6_3fact_white.png">
</p>
</figure>

<p>
we reject the null and conclude that at least one of the coefficients is nonzero, indicating that we have heteroskedasticity present. the test statistic here is on the second line and has a \(\chi^2(14)\) distribution under the null
</p>

<p>
we should also test for serial correlation between the residuals
</p>


<figure>
<p><img src="./images/tut6_3fact_arch_window.png" class="img-responsive" alt="tut6_3fact_arch_window.png">
</p>
</figure>


<figure>
<p><img src="./images/tut6_3fact_arch.png" class="img-responsive" alt="tut6_3fact_arch.png">
</p>
</figure>

<p>
again the test statistic is \(TR^2\) on the second line, again we see that we reject the null hypothesis
</p>

<div class="org-src-container">

<pre class="src src-Eviews">@qchisq(0.95, 10)
</pre>
</div>

<ol class="org-ol">
<li>non-normal residuals isnt too bad bc we can go asymptotic
</li>
<li>heteroskedasticity and serial correlation is pretty bad and ruins the standard errors so our hypothesis tests are junk
</li>
</ol>

<p>
we will redo our regression using robust standard errors
</p>


<figure>
<p><img src="./images/tut6_3fact_hac.png" class="img-responsive" alt="tut6_3fact_hac.png">
</p>
</figure>

<p>
we now cannot reject the null that the coefficient on smb is statistically different from zero
</p>

<hr >


<figure>
<p><img src="./images/tut6_health_serial.png" class="img-responsive" alt="tut6_health_serial.png">
</p>
</figure>


<figure>
<p><img src="./images/tut6_health_arch.png" class="img-responsive" alt="tut6_health_arch.png">
</p>
</figure>

<p>
we cant predict if it's going up or down but we probably can predict if there is about to be a big move or the volatility of the series
</p>

<p>
lets look at the absolute values
</p>

<div class="org-src-container">

<pre class="src src-EViews">series health_abs = @abs(hlth - @mean(hlth))
</pre>
</div>

<p>
and we'll look at a correlogram on these absolute returns
</p>


<figure>
<p><img src="./images/tut6_health_abs_correl.png" class="img-responsive" alt="tut6_health_abs_correl.png">
</p>
</figure>

<p>
although we cannot predict the direction we tend to be able to predict the magnitude of returns
</p>

<p>
for the other portfolios, we summarise the results as follows
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="left">

<col  class="left">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Portfolio</th>
<th scope="col" class="text-left">Serial correlation</th>
<th scope="col" class="text-left">ARCH effects</th>
<th scope="col" class="text-left">Absoute returns persistent</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">Health</td>
<td class="text-left">No (p=0.0650)</td>
<td class="text-left">Yes (p=0.0000)</td>
<td class="text-left">Yes (about first 5 lags)</td>
</tr>

<tr>
<td class="text-left">Consumer</td>
<td class="text-left">Yes (p=0.0014)</td>
<td class="text-left">Yes (p=0.0000)</td>
<td class="text-left">Yes (about first 10 lags)</td>
</tr>

<tr>
<td class="text-left">Manufacturing</td>
<td class="text-left">No (p=0.5550)</td>
<td class="text-left">Yes (p=0.0000)</td>
<td class="text-left">Yes (about first 15 lags)</td>
</tr>

<tr>
<td class="text-left">Tech</td>
<td class="text-left">Yes (p=0.0422)</td>
<td class="text-left">Yes (p=0.0000)</td>
<td class="text-left">Yes (about first 10 lags)</td>
</tr>

<tr>
<td class="text-left">Other</td>
<td class="text-left">No (p=0.0653)</td>
<td class="text-left">Yes (p=0.0000)</td>
<td class="text-left">Yes (about first 5 lags)</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-sec-3-6" class="outline-3">
<h3 id="sec-3-6"><span class="section-number-3">3.6</span> <span class="label label-primary PROGRESS">PROGRESS</span> Week 7 laboratory</h3>
<div class="outline-text-3" id="text-3-6">
</div><div id="outline-container-sec-3-6-1" class="outline-4">
<h4 id="sec-3-6-1"><span class="section-number-4">3.6.1</span> Notes on choosing between AR and MA models</h4>
<div class="outline-text-4" id="text-3-6-1">
<ul class="org-ul">
<li>If there looks like there are slowly decaying autocorrelation then prefer AR
</li>
<li>If there are a few fixed autocorrelations then nothing, prefer MA
</li>
<li>We require covariance stationarity to do anything useful
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3-6-2" class="outline-4">
<h4 id="sec-3-6-2"><span class="section-number-4">3.6.2</span> Question one</h4>
<div class="outline-text-4" id="text-3-6-2">
<p>
Let \(y_t\) be a MA(1) process
</p>

<p>
\[ y_t - \mu = \varepsilon_t - \theta \varepsilon_{t-1} \]
</p>

<blockquote>
<p>
Show that \(\mathbb{E}[y_t] = \mu\)
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
\mathbb{E}[y_t] &=& \mathbb{E}[\mu + \varepsilon_t - \theta \varepsilon_{t-1}] \\
&=& \mathbb{E}[\mu] + \mathbb{E}[\varepsilon_t] - \theta \mathbb{E}[\varepsilon_{t-1}] \\
&=& \mu + 0 - 0 = \mu \text{ as } \varepsilon_t \overset{\text{i.i.d.}}{\sim} \text{WN}(0, \sigma^2) \\
\end{array} \]
</p>

<blockquote>
<p>
Show that \(\mathbb{V}(y_t) = \sigma^2(1+\theta^2)\)
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
\mathbb{V}(y_t) &=& \mathbb{V}(\mu + \varepsilon_t - \theta \varepsilon_{t-1}) \\
&=& \mathbb{V}(\varepsilon_t - \theta \varepsilon_{t-1}) \\
&=& \mathbb{V}(\varepsilon_t) + \theta^2 \mathbb{V}(\varepsilon_{t-1}) \\
&=& \mathbb{V}(\varepsilon_t) + \theta^2 \mathbb{V}(\varepsilon_t) \text{ as } \varepsilon_t \overset{\text{i.i.d.}}{\sim} \text{WN}(0, \sigma^2) \\
&=& \sigma^2 + \theta^2 \sigma^2 \\
&=& \sigma^2 (1 + \theta^2) \\
\end{array} \]
</p>

<blockquote>
<p>
Show that \(\gamma(1) = \text{Cov}(y_t, y_{t-1}) = - \theta \sigma^2\)
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
\gamma(1) &=& \text{Cov}(y_t, y_{t-1}) \\
&=& \mathbb{E}[y_t y_{t-1}] - \mathbb{E}[y_t] \mathbb{E}[y_{t-1}] \\
&=& \mathbb{E}[(\mu + \varepsilon_t - \theta \varepsilon_{t-1}) (\mu + \varepsilon_{t-1} - \theta \varepsilon_{t-2})] - \mu^2 \\
&=& \mathbb{E}[\mu^2 + \mu \varepsilon_{t-1} - \mu \theta \varepsilon_{t-2} + \varepsilon_t \mu + \varepsilon_t \varepsilon_{t-1} - \varepsilon_t \theta \varepsilon_{t-2} - \theta \varepsilon_{t-1} \mu - \theta \varepsilon_{t-1} \varepsilon_{t-1} + \theta^2 \varepsilon_{t-1} \varepsilon_{t-2}] - \mu^2 \\
&=& \mu^2 - \theta\mathbb{E}[\varepsilon_{t-1}^2] - \mu^2 \text{ as } \varepsilon_t \overset{\text{i.i.d.}}{\sim} \text{WN}(0, \sigma^2) \\
&=& - \theta\mathbb{E}[\varepsilon_{t-1}^2] \\
&=& - \theta \sigma^2 \\
\end{array} \]
</p>

<blockquote>
<p>
Show that \(\gamma(h) = \text{Cov}(y_t, y_{t-h}) = 0\) when \(h>1\)
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
\gamma(h) &=& \text{Cov}(y_t, y_{t-h}) \\
&=& \mathbb{E}[y_t y_{t-h}] - \mathbb{E}[y_t] \mathbb{E}[y_{t-h}] \\
&=& \mathbb{E}[(\mu + \varepsilon_t - \theta \varepsilon_{t-1}) (\mu + \varepsilon_{t-h} - \theta \varepsilon_{t-h-1})] - \mu^2 \\
&=& \mathbb{E}[\mu^2 + \mu \varepsilon_{t-h} - \mu \theta \varepsilon_{t-h-1} + \varepsilon_t \mu + \varepsilon_t \varepsilon_{t-h} - \varepsilon_t \theta \varepsilon_{t-h-1} - \theta \varepsilon_{t-1} \mu - \theta \varepsilon_{t-1} \varepsilon_{t-h} + \theta^2 \varepsilon_{t-1} \varepsilon_{t-h-1}] - \mu^2 \\
&=& \mu^2 - \mu^2 \text{ as } \varepsilon_t \overset{\text{i.i.d.}}{\sim} \text{WN}(0, \sigma^2) \text{ so } \mathbb{E}[\varepsilon_i \varepsilon_j] = 0 \text{ when } i \not= j \\
&=& 0 \\
\end{array} \]
</p>

<p>
We note that since a MA(q) model will only have nonzero autocorrelation up to order q, we should see this pattern in our data for it to be a viable option for modelling. We will look at BHP returns to examine if an MA(1) or MA(2) model would be viable.
</p>


<figure>
<p><img src="./images/week7tut_bhp_hist.png" class="img-responsive" alt="week7tut_bhp_hist.png" width="700px">
</p>
</figure>

<p>
Looking at the distribution of returns, we note that they have excess kurtosis and some negative skewness. Jarque-Bera statistic rejects the null hypothesis of normality. In the correlogram we see
</p>


<figure>
<p><img src="./images/week7tut_bhp_correl.png" class="img-responsive" alt="week7tut_bhp_correl.png" width="500px">
</p>
</figure>

<p>
We estimate a MA(1) and a MA(2) model.
</p>


<figure>
<p><img src="./images/week7tut_bhp_ma.png" class="img-responsive" alt="week7tut_bhp_ma.png" width="800px">
</p>
</figure>

<p>
We can see that the second MA term is insignificant at any reasonable significance level (can test this using a T-test).
</p>
</div>
</div>

<div id="outline-container-sec-3-6-3" class="outline-4">
<h4 id="sec-3-6-3"><span class="section-number-4">3.6.3</span> <span class="label label-primary PROGRESS">PROGRESS</span> Question two</h4>
<div class="outline-text-4" id="text-3-6-3">
<p>
Let \(y_t\) be a ARMA(1,1) process
</p>

<p>
\[ y_t - \mu = \phi_1 y_{t-1} + \varepsilon_t - \theta \varepsilon_{t-1} \]
</p>

<blockquote>
<p>
Find \(\mathbb{E}[y_t]\)
</p>
</blockquote>

<p>
Did this on paper, too much algebra to type up
</p>

<p>
\[ \mathbb{E}[y_t] = \frac{\mu}{1 - \phi_1} \]
</p>

<blockquote>
<p>
Find \(\mathbb{V}(y_t)\)
</p>
</blockquote>

<p>
Did this on paper, too much algebra to type up
</p>

<p>
\[ \mathbb{V}(y_t) = \frac{\sigma^2 (1 + \theta^2 + 2 \phi_1)}{1 - \phi_1^2} \]
</p>

<blockquote>
<p>
Find \(\gamma(1) = \mathbb{E}[y_t y_{t-1}] - \mathbb{E}[y_t] \mathbb{E}[y_{t-1}]\)
</p>
</blockquote>

<p>
Very messy, didn't quite get through this one
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-7" class="outline-3">
<h3 id="sec-3-7"><span class="section-number-3">3.7</span> <span class="label label-primary TODO">TODO</span> Week 8 laboratory</h3>
<div class="outline-text-3" id="text-3-7">
</div><div id="outline-container-sec-3-7-1" class="outline-4">
<h4 id="sec-3-7-1"><span class="section-number-4">3.7.1</span> <span class="label label-primary TODO">TODO</span> Question One</h4>
<div class="outline-text-4" id="text-3-7-1">
<ol class="org-ol">
<li>log likelihood much higher for daily returns
</li>
<li>The daily returns seem to have fit better
</li>
<li>Using 20 lags ARCH effects are present in both models
</li>
<li>go to estimate tab, change estimation setting to use arch with order 2 arch and order 0 garch
</li>
<li>there's still autocorrelation present in both return series for arch(2) errors
</li>
<li>same process as before
</li>
<li>with arch(5) monthly returns no longer have arch effects but still present for daily returns, same with arch(7)
</li>
<li>need more arch terms for daily
</li>
<li>more patterns in daily data compared to monthly data
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-3-7-2" class="outline-4">
<h4 id="sec-3-7-2"><span class="section-number-4">3.7.2</span> <span class="label label-primary TODO">TODO</span> Question Two</h4>
<div class="outline-text-4" id="text-3-7-2">
<p>
This is mostly covered in W8L1
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-8" class="outline-3">
<h3 id="sec-3-8"><span class="section-number-3">3.8</span> Week 9 laboratory</h3>
<div class="outline-text-3" id="text-3-8">
</div><div id="outline-container-sec-3-8-1" class="outline-4">
<h4 id="sec-3-8-1"><span class="section-number-4">3.8.1</span> Question one</h4>
<div class="outline-text-4" id="text-3-8-1">
<p>
Using the excess returns on the consumer portfolio, we willl fit a three factor CAPM with a constant variance model, ARCH(1) errors and ARCH(5) errors.
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="right">

<col  class="left">

<col  class="left">

<col  class="right">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Model</th>
<th scope="col" class="text-right">Significant lags in correl</th>
<th scope="col" class="text-left">Serial correlation</th>
<th scope="col" class="text-left">ARCH effects (order 5)</th>
<th scope="col" class="text-right">Log likelihood</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">CAPM</td>
<td class="text-right">2</td>
<td class="text-left">Yes</td>
<td class="text-left">Yes</td>
<td class="text-right">-2133.642</td>
</tr>

<tr>
<td class="text-left">CAPM with ARCH(1) errors</td>
<td class="text-right">2</td>
<td class="text-left">&#xa0;</td>
<td class="text-left">Yes</td>
<td class="text-right">-2091.202</td>
</tr>

<tr>
<td class="text-left">CAPM with ARCH(5) errors</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-left">No</td>
<td class="text-right">-2042.194</td>
</tr>
</tbody>
</table>

<p>
When using OLS regression, R<sup>2</sup> is a useful measure of fit but loses interpretation for ARCH and GARCH models which use gradient optimizers to maximise likelihood. To decide between the ARCH(1) an ARCH(5) models, we can use a LR test. The test statistic is
</p>

<p>
\[ LR = 2 (-2042 + 2091) \approx 98 \]
</p>

<p>
Since we are testing four restrictions (i.e. whether the second to fourth order ARCH terms are needed), \(LR \sim \chi^2_4\).
</p>

<div class="org-src-container">

<pre class="src src-R">paste<span style="color: #729fcf;">(</span><span style="color: #de7fa8;">"The critical value is"</span>, round<span style="color: #8ae234;">(</span>qchisq<span style="color: #fce94f;">(</span><span style="color: #73d216;">0.99</span>, df=<span style="color: #73d216;">4</span><span style="color: #fce94f;">)</span>, <span style="color: #73d216;">2</span><span style="color: #8ae234;">)</span><span style="color: #729fcf;">)</span>
</pre>
</div>

<pre class="example">
The critical value is 13.28

</pre>

<p>
The critical value for the 1% level is 13.28. Therefore we reject the null hypothesis at the 1% level. ARCH(1) errors are insufficient for the three factor CAPM on the consumer portfolio.
</p>

<p>
We can repeat this process with the hi-tech portfolio.
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="right">

<col  class="left">

<col  class="left">

<col  class="right">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Model</th>
<th scope="col" class="text-right">Significant lags in correl</th>
<th scope="col" class="text-left">Serial correlation</th>
<th scope="col" class="text-left">ARCH effects (order 5)</th>
<th scope="col" class="text-right">Log likelihood</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">CAPM</td>
<td class="text-right">0</td>
<td class="text-left">Yes</td>
<td class="text-left">Yes</td>
<td class="text-right">-2578.488</td>
</tr>

<tr>
<td class="text-left">CAPM with ARCH(1) errors</td>
<td class="text-right">0</td>
<td class="text-left">&#xa0;</td>
<td class="text-left">Yes</td>
<td class="text-right">-2545.059</td>
</tr>

<tr>
<td class="text-left">CAPM with ARCH(5) errors</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-left">No</td>
<td class="text-right">-2516.701</td>
</tr>
</tbody>
</table>

<p>
\[ LR = 2 (-2516.701 + 2545.059) \approx 56 \]
</p>

<p>
ARCH(5) is preferred to ARCH(1) again. We note that \(\alpha\) is insignificantly different from 0 in this portfolio on all three models, while for the consumer portfolio we saw strong evidence that \(\alpha > 0\).
</p>
</div>
</div>

<div id="outline-container-sec-3-8-2" class="outline-4">
<h4 id="sec-3-8-2"><span class="section-number-4">3.8.2</span> Diagnostic tests</h4>
<div class="outline-text-4" id="text-3-8-2">
<ol class="org-ol">
<li>Diagnostics for mean value model (IS there ARCH effects in the model)
<ol class="org-ol">
<li>Run mean model (constant mean, ARMA &#x2026;)
</li>
<li>estimate \(\hat{\varepsilon}_t^2 = \gamma_0 + \gamma_1 \hat{\varepsilon}_{t-1}^2 + ... + v_t\)
</li>
<li>using LM test with \(TR^2\) from auxiliary regression
</li>
</ol>
</li>

<li>Diagnostic test for ARCH/GARCH model (Is there left over ARCH effects)
<ol class="org-ol">
<li>Estimate mean model (constant mean, ARMA, CAPM) and volatility model
</li>
<li>The residuals \(\varepsilon_t\) are in the form \(\varepsilon_t = u_t \sigma_t\)
</li>
<li>We can specify the distribution of \(u_t\) (often this is simply a conditional normal assumption given \(\mathcal{F})\)
</li>
<li>We estimate the auxiliary regression \(\hat{u}_t^2 = \gamma_0 + \gamma_1 \hat{u}_{t-1}^2 + ... + v_t\)
</li>
<li>Intuition: we test the standardised residuals because assume that we perfectly fit the volatility with ARCH, then if we test the residuals we still get noisy arch effects but if we scale them with what we predicted volatility to be we get constant mean and variance.
</li>
</ol>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-sec-3-9" class="outline-3">
<h3 id="sec-3-9"><span class="section-number-3">3.9</span> Week 10 laboratory</h3>
<div class="outline-text-3" id="text-3-9">
</div><div id="outline-container-sec-3-9-1" class="outline-4">
<h4 id="sec-3-9-1"><span class="section-number-4">3.9.1</span> Question one</h4>
<div class="outline-text-4" id="text-3-9-1">
<p>
We estimate a three-factor CAPM model on consumer returns from 1926/7 to 2019/2. We use one model with ARCH(2) errors and another with GARCH(1,1) errors.
</p>


<figure>
<p><img src="./images/tut10_models.png" class="img-responsive" alt="tut10_models.png" width="800px">
</p>
</figure>

<p>
We cannot complete a likelihood ratio rest between these two models since they are not nested models (i.e. neither model is a direct extension of the other). The AIC value for the model with ARCH(2) errors (\(k=7\), \(l=-2198.729\), \(T=1112\)) is 3.967. The AIC for the model with GARCH(1,1) errors (\(k=7\), \(l=-2156.412\), \(T=1112\)) is 3.891. Based on this criteria, the GARCH(1,1) model is preferred.
</p>

<p>
Next we estimate a model with EGARCH(1,1) errors.
</p>


<figure>
<p><img src="./images/tut10_egarch1_1.png" class="img-responsive" alt="tut10_egarch1_1.png" width="400px">
</p>
</figure>

<p>
The model with EGARCH(1,1) errors (\(k=8\), \(l=-2148.831\), \(T=1112\)) has an AIC of 3.879. Despite the extra degree of freedom, based on AIC the EGARCH(1,1) model is preferable to the GARCH(1,1) and ARCH(2) models.
</p>
</div>
</div>

<div id="outline-container-sec-3-9-2" class="outline-4">
<h4 id="sec-3-9-2"><span class="section-number-4">3.9.2</span> Question two</h4>
<div class="outline-text-4" id="text-3-9-2">
<p>
In this question we generate returns sampled from a GARCH(1,1) process. In EViews we can use a program to simulate the process for us.
</p>

<div class="org-src-container">

<pre class="src src-EVIEWS">genr ut = nrnd
scalar n = 1112

scalar alpha0 = 0.01
scalar alpha1 = 0.05
scalar beta = 0.9

series sigmaSq = alpha0 / (1 - alpha1 - beta)
series r = @sqrt(sigmaSq) * ut

for !i=1 to n-1
smpl @first+!i @first+!i
sigmaSq = alpha0 + alpha1*r(-1)^2 + beta*sigmaSq(-1)
r = @sqrt(sigmaSq) * ut
next

smpl @all
</pre>
</div>

<p>
Running this generates a series <code>sigmaSq</code> for the volatility and <code>r</code> for the returns. Sure enough, fitting a GARCH(1, 1) model to <code>r</code> closely recovers the true coefficients of the process.
</p>


<figure>
<p><img src="./images/tut10_recovered_garch1_1.png" class="img-responsive" alt="tut10_recovered_garch1_1.png" width="400px">
</p>
</figure>

<p>
The correlogram shows very little autocorrelation in the returns (i.e. that the return series is not persistent).
</p>


<figure>
<p><img src="./images/tut10_garch1_1_correl.png" class="img-responsive" alt="tut10_garch1_1_correl.png" width="400px">
</p>
</figure>

<p>
In the sample generated we see slight positive skew in the returns and kurtosis of nearly 3. The Jarque-Bera test fails to reject the null hypothesis of normality at a 10% confidence level.
</p>


<figure>
<p><img src="./images/tut10_garch1_1_dist.png" class="img-responsive" alt="tut10_garch1_1_dist.png" width="600px">
</p>
</figure>

<p>
Repeating this process with longer return series (e.g. \(n=25000\)) generates a series with 0 skewness and greater excess kurtosis (about 3.12) and the Jarque-Bera test rejects the null hypothesis of normality.
</p>
</div>
</div>

<div id="outline-container-sec-3-9-3" class="outline-4">
<h4 id="sec-3-9-3"><span class="section-number-4">3.9.3</span> Question three</h4>
<div class="outline-text-4" id="text-3-9-3">
<p>
In this question we simulate a TGARCH(1,1) process. This is done in much the same way as the previous question.
</p>

<div class="org-src-container">

<pre class="src src-EVIEWS">genr ut = nrnd
scalar n = 25000

scalar alpha0 = 0.00003
scalar alpha1 = 0.05
scalar alpha2 = 0.05
scalar beta = 0.83

series sigmaSq = alpha0 / (1 - (alpha1 + alpha2/2) - beta)
series r = @sqrt(sigmaSq) * ut

for !i=1 to n-1
smpl @first+!i @first+!i
sigmaSq = alpha0 + alpha1*@abs(r(-1)) + alpha2*@recode(r(-1)&lt;0, 1, 0)*@abs(r(-1)) + beta*sigmaSq(-1)
r = @sqrt(sigmaSq) * ut
next

smpl @all
</pre>
</div>

<p>
A sample of 25000 periods drawn from this process shows a slight positive skew and kurtosis of about 3.4.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-10" class="outline-3">
<h3 id="sec-3-10"><span class="section-number-3">3.10</span> Week 11 laboratory</h3>
<div class="outline-text-3" id="text-3-10">
</div><div id="outline-container-sec-3-10-1" class="outline-4">
<h4 id="sec-3-10-1"><span class="section-number-4">3.10.1</span> Question one</h4>
<div class="outline-text-4" id="text-3-10-1">
<p>
The sample mean can be calculated as follows.
</p>

<p>
\[ \mu = \frac{1}{T} \sum_{t=1}^T r_t \]
</p>

<p>
The sample variance can be calculated as follows.
</p>

<p>
\[ \bar{\mu}_2 = \frac{1}{T} \sum_{t=1}^T (r_t - \mu)^2 \]
</p>

<p>
The sample skewness can be calculated as follows.
</p>

<p>
\[ \bar{\mu}_3^s = \frac{1}{T} \sum_{t=1}^T \frac{(r_t - \mu)^3}{\bar{\mu}_2} \]
</p>

<p>
The sample kurtosis can be calculated as follows.
</p>

<p>
\[ \bar{\mu}_4^s = \frac{1}{T} \sum_{t=1}^T \frac{(r_t - \mu)^4}{\bar{\mu}_2} \]
</p>
</div>
</div>

<div id="outline-container-sec-3-10-2" class="outline-4">
<h4 id="sec-3-10-2"><span class="section-number-4">3.10.2</span> Question two</h4>
<div class="outline-text-4" id="text-3-10-2">
<p>
Under the null hypothesis that the data follows a normal distribution, the population skewness is equal to 0 and the population kurtosis is equal to 3.
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& SK = 0 \text{ and } KT = 3 \\
H_1 &:& \text{Either } SK \neq 0 \text{ or } KT \neq 3 \\
\end{array} \]
</p>

<p>
Under the null hypothesis the \(JB\) test statistic follows a chi-squared distribution with two degrees of freedom. Therefore the critical value at a 95% significance level is 5.99. We reject the null hypothesis when \(JB > 5.99\) and fail to reject when \(JB \leq 5.99\).
</p>
</div>
</div>

<div id="outline-container-sec-3-10-3" class="outline-4">
<h4 id="sec-3-10-3"><span class="section-number-4">3.10.3</span> Question three</h4>
<div class="outline-text-4" id="text-3-10-3">
<p>
Let \(X = (X_1, X_2)^{\prime}\) be a bivariate normal random vector
</p>

<p>
\[ X = \begin{bmatrix}
X_1 \\ X_2
\end{bmatrix} \sim \mathcal{N} \left (\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} , \begin{bmatrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{21} & \sigma_2^2 \\ \end{bmatrix} \right ) \]
</p>

<p>
The conditional distribution of \(X_1\) given \(X_2\) is as follows
</p>

<p>
\[ X_1 | X_2 = x_2 \sim \mathcal{N} \left ( \mu_1 + \frac{\sigma_{12}}{\sigma_2^2}(x_2 - \mu_2), \frac{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2}{\sigma_2^2} \right ) \]
</p>
</div>
</div>

<div id="outline-container-sec-3-10-4" class="outline-4">
<h4 id="sec-3-10-4"><span class="section-number-4">3.10.4</span> Question four</h4>
<div class="outline-text-4" id="text-3-10-4">
<blockquote>
<p>
What is the distribution of the \(k\) period log return \(r_t(k)\)?
</p>
</blockquote>

<p>
Since the log returns across all periods are i.i.d., we have the sum of \(k\) indepdent normals with identical distributions. It follows then that
</p>

<p>
\[ r_t(k) \sim \mathcal{N}(k \mu, k \sigma^2) \]
</p>

<blockquote>
<p>
What is the distribution of the gross return \(1 + R_t\)?
</p>
</blockquote>

<p>
Recall that \(r_t = \log(1+R_t)\). Since \(r_t \sim \mathcal{N}(\mu, \sigma^2)\) we can write
</p>

<p>
\[ r_t = \mu + \sigma Z \]
</p>

<p>
for \(Z \sim \mathcal{N}(0,1)\). Therefore
</p>

<p>
\[ 1 + R_t = exp(\mu + \sigma Z) \sim \text{lognormal}(\mu, \sigma^2) \]
</p>

<blockquote>
<p>
Derive the formula for probability \(\mathbb{P}(1 + R_t < x)\)
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
\mathbb{P}(1 + R_t < x) &=& \mathbb{P}(\log(1 + R_t) < \log(x))  \\
&=& \mathbb{P}(r_t < \log(x))  \\
&=& \displaystyle \mathbb{P}\left ( \frac{r_t - \mu}{\sigma} < \frac{\log(x) - \mu}{\sigma} \right)  \\
&=& \displaystyle \mathbb{P}\left ( Z < \frac{\log(x) - \mu}{\sigma} \right)  \\
&=& \displaystyle \Theta \left (\frac{\log(x) - \mu}{\sigma} \right)  \\
\end{array} \]
</p>
</div>
</div>

<div id="outline-container-sec-3-10-5" class="outline-4">
<h4 id="sec-3-10-5"><span class="section-number-4">3.10.5</span> Question five</h4>
<div class="outline-text-4" id="text-3-10-5">
<blockquote>
<p>
State the procedure for testing the validity of a single factor CAPM
</p>
</blockquote>

<ol class="org-ol">
<li>Estimate the single-factor CAPM
</li>
<li>Construct the following auxiliary regression equation to test the following set of hypotheses
</li>
</ol>

<p>
\[ \hat{\varepsilon}_{it} = \beta_2 \text{SMB}_t + \beta_3 \text{HML}_t + v_{it} \]
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \beta_2 = \beta_3  = 0 \text{, the single factor CAPM is valid} \\
H_1 &:& \text{At least one of } \beta_2, \beta_3 \text{ is non-zero, the single factor CAPM is invalid} \\
\end{array} \]
</p>

<ol class="org-ol">
<li value="3">Obtain the \(R^2\) and calculate the test statistic \(TR^2\) which has a \(\chi^2_2\) distribution under \(H_0\)
</li>
<li>Calculate the critical value at an appropriate level and conduct the test
</li>
</ol>

<blockquote>
<p>
Using an LM test, set out a testing procedure to test the following null hypothesis
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \text{The errors are homoskedastic} \\
H_1 &:& \text{The errors are heteroskedastic} \\
\end{array} \]
</p>
</blockquote>

<p>
Use an LM test with the following auxiliary regression equation
</p>

<p>
\[ \hat{\varepsilon}_{it} = c +
\gamma_1 (r_{mt} - r_{ft}) + \gamma_2 (r_{mt} - r_{ft})^2 +
\beta_3 \text{SMB}_t + \beta_4 \text{SMB}_t^2 +
\beta_5 \text{HML}_t + \beta_6 \text{HML}_t^2 +
v_{it} \]
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \gamma_1 = \cdots = \gamma_6 = 0 \\
H_1 &:& \text{At least one of } \gamma_1, \ldots, \gamma_6 \text{ is nonzero} \\
\end{array} \]
</p>

<p>
This is commonly known as White's test. Under \(H_0\), \(TR^2 \sim \chi^2_6\).
</p>

<blockquote>
<p>
Using an LM test, set out a testing procedure to test the following null hypothesis
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \text{There are no ARCH effects} \\
H_1 &:& \text{There are ARCH effects} \\
\end{array} \]
</p>
</blockquote>

<p>
Use an LM test with the following auxiliary regression equation
</p>

<p>
\[ \hat{\varepsilon}_{it} = c +
\gamma_1 \hat{\varepsilon}_{i, t-1} +
\gamma_2 \hat{\varepsilon}_{i, t-2} +
\gamma_3 \hat{\varepsilon}_{i, t-3} +
\gamma_4 \hat{\varepsilon}_{i, t-4} +
\gamma_5 \hat{\varepsilon}_{i, t-5} +
v_{it} \]
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \gamma_1 = \cdots = \gamma_5 = 0 \\
H_1 &:& \text{At least one of } \gamma_1, \ldots, \gamma_5 \text{ is nonzero} \\
\end{array} \]
</p>

<p>
Under \(H_0\), \(TR^2 \sim \chi^2_5\). This tests for ARCH effects up to order 5.
</p>
</div>
</div>

<div id="outline-container-sec-3-10-6" class="outline-4">
<h4 id="sec-3-10-6"><span class="section-number-4">3.10.6</span> <span class="label label-primary TODO">TODO</span> Question five (extra)</h4>
</div>
<div id="outline-container-sec-3-10-7" class="outline-4">
<h4 id="sec-3-10-7"><span class="section-number-4">3.10.7</span> Question six</h4>
<div class="outline-text-4" id="text-3-10-7">
<blockquote>
<p>
What assumptions about the error term \(\varepsilon_t\) are requried for the efficient market hypothesis (EMH) to be satisfied?
</p>
</blockquote>

<p>
Suppose we have the model of returns \(r_t\) as follows
\[ r_t = \mathbb{E}[r_t | \mathcal{F}_{t-1}] + \varepsilon_t \]
</p>

<p>
We require \(\varepsilon_t \sim \text{WN}(0, \sigma^2_{\varepsilon})\), i.e. that we have zero mean, constant variance and no autocorrelation in the residual series.
</p>

<blockquote>
<p>
If the EMH is satisfied, what does this mean about the predictability of returns?
</p>
</blockquote>

<p>
Conditional on all past information, future returns are completely random and therefore unpredictable. We cannot do better than expecting the conditional mean and variance.
</p>

<blockquote>
<p>
State the steps necessary to test the EMH using \(\rho(1)\)
</p>
</blockquote>

<p>
Under the null hypothesis of EMH, we should expect that the correlation between the current return and the return 1 period ago should be zero. Therefore we can test the hypothesis that \(\rho(1)=0\) against the alternative that \(\rho(1)\not=0\). Under \(H_0\), \(\rho(1) \sim \mathcal{N}(0, 1/T)\) so our test statistic for a two sided T test is \(abs(\sqrt{T}\hat{\rho}(1))\). A rejection implies that returns are predictable over that given time period and that the EMH is invalidated.
</p>

<blockquote>
<p>
State the steps necessary to test the EMH using an auxiliary regression of the residuals
</p>
</blockquote>

<p>
We can construct a more comprehensive test to determine whether any lags are found to be significant predictors of future returns. To test up to order \(k\) we construct the auxiliary regression equation
</p>

<p>
\[ \hat{\varepsilon}_t = \gamma_0 + \sum_{l=1}^k \gamma_l \hat{\varepsilon}_{t-l} + v_t \]
</p>

<p>
This is an LM test with the following hypotheses
</p>

<p>
\[ \begin{array}{rcl}
H_0 &:& \gamma_1 = \cdots = \gamma_k = 0 \text{, EMH is satisfied} \\
H_1 &:& \text{At least one of } \gamma_1, \ldots, \gamma_k \text{is non-zero, EMH is not satisfied} \\
\end{array} \]
</p>

<p>
This has a test statistic of \(TR^2 \sim \chi^2_k\) under \(H_0\).
</p>

<blockquote>
<p>
If we reject EMH, how can we improve the model?
</p>
</blockquote>

<p>
We can model the dynamics of returns by using more complicated mean models such as \(\text{AR}(p)\) or \(\text{ARMA}(p, q)\) models.
</p>
</div>
</div>

<div id="outline-container-sec-3-10-8" class="outline-4">
<h4 id="sec-3-10-8"><span class="section-number-4">3.10.8</span> Question seven</h4>
<div class="outline-text-4" id="text-3-10-8">
<blockquote>
<p>
Derive the unconditional expectation of \(r_t\), an AR(1) process.
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
\mathbb{E}[r_t] &=& \mathbb{E}[\phi_0 + \phi_1 r_{t-1} + \varepsilon_t] \\
&=& \mathbb{E}[\phi_0] + \mathbb{E}[\phi_1 r_{t-1}] + \mathbb{E}[\varepsilon_t] \\
&=& \phi_0 + \phi_1 \mathbb{E}[r_{t-1}] + 0 \\
&=& \phi_0 + \phi_1 \mathbb{E}[r_t] \text{, assuming that } r_t \text{ is covariance stationary} \\
\mathbb{E}[r_t] - \phi_1 \mathbb{E}[r_t] &=& \phi_0 \\
\therefore \mathbb{E}[r_t] &=& \displaystyle \frac{\phi_0}{1 - \phi_1} \\
\end{array} \]
</p>

<blockquote>
<p>
Derive the unconditional variance of \(r_t\), an AR(1) process.
</p>
</blockquote>

<p>
\[ \begin{array}{rcl}
\mathbb{V}(r_t) &=& \mathbb{V}(\phi_0 + \phi_1 r_{t-1} + \varepsilon_t) \\
&=& \mathbb{V}(\phi_1 r_{t-1} + \varepsilon_t) \\
&=& \phi_1^2 \mathbb{V}(r_{t-1}) + \mathbb{V}(\varepsilon_t) + 2\phi_1 \text{Cov}(r_{t-1}, \varepsilon_t) \\
&=& \phi_1^2 \mathbb{V}(r_{t-1}) + \sigma^2 + 0 \text{, as } \varepsilon_t \sim \text{WN}(0, \sigma^2) \\
&=& \phi_1^2 \mathbb{V}(r_t) + \sigma^2 \text{, assuming that } r_t \text{ is covariance stationary} \\
\therefore \mathbb{V}(r_t) &=& \displaystyle \frac{\sigma^2}{1 - \phi_1^2} \\
\end{array} \]
</p>

<blockquote>
<p>
Derive the unconditional correlation between \(r_t\) and \(r_{t-1}\) where \(r_t\) is an AR(1) process
</p>
</blockquote>

<p>
Recall that
</p>

<p>
\[ \text{Corr}(r_t, r_{t-1}) = \frac{\text{Cov}(r_t, r_{t-1})} {\sqrt{ \mathbb{V}(r_t) \mathbb{V}(r_{t-1}) } } \]
</p>

<p>
We first derive the covariance
</p>

<p>
\[ \begin{array}{rcl}
\text{Cov}(r_t, r_{t-1}) &=& \text{Cov}(\phi_0 + \phi_1 r_{t-1} + \varepsilon_t, r_{t-1}) \\
&=& \text{Cov}(\phi_0, r_{t-1}) + \text{Cov}(\phi_1 r_{t-1}, r_{t-1}) + \text{Cov}(\varepsilon_t, r_{t-1}) \\
&=& 0 + \text{Cov}(\phi_1 r_{t-1}, r_{t-1}) + 0 \\
&=& \phi_1 \mathbb{V}(r_{t-1}) \\
&=& \phi_1 \mathbb{V}(r_t) \text{, assuming that } r_t \text{ is covariance stationary} \\
\end{array} \]
</p>

<p>
But then we also have \(\mathbb{V}(r_t) = \mathbb{V}(r_{t-1})\) when \(r_t\) is covariance stationary, therefore
</p>

<p>
\[ \begin{array}{rcl}
\text{Corr}(r_t, r_{t-1}) &=& \displaystyle \frac{\text{Cov}(r_t, r_{t-1})} {\sqrt{ \mathbb{V}(r_t) \mathbb{V}(r_{t-1}) } } \\
&=& \displaystyle \frac{\phi_1 \mathbb{V}(r_t)} {\sqrt{ \mathbb{V}(r_t) \mathbb{V}(r_t) } } \\
\therefore \text{Corr}(r_t, r_{t-1}) &=& \phi_1 \\
\end{array} \]
</p>
</div>
</div>
</div>
</div>
</div><div class="col-md-3"><nav id="table-of-contents">
<div id="text-table-of-contents" class="bs-docs-sidebar">
<ul class="nav">
<li><a href="#sec-1">1. Unit guide summary</a>
<ul class="nav">
<li><a href="#sec-1-1">1.1. Synopsis</a></li>
<li><a href="#sec-1-2">1.2. Assessment summary</a></li>
<li><a href="#sec-1-3">1.3. Unit schedule</a></li>
</ul>
</li>
<li><a href="#sec-2">2. Lectures</a>
<ul class="nav">
<li><a href="#sec-2-1">2.1. Lecture 1</a></li>
<li><a href="#sec-2-2">2.2. Lecture 2 - Probability and statistics review</a></li>
<li><a href="#sec-2-3">2.3. Lecture 3 - Probability and statistics review</a></li>
<li><a href="#sec-2-4">2.4. Lecture 4 - Multivariate distributions</a></li>
<li><a href="#sec-2-5">2.5. Lecture 5 - Financial assets and returns</a></li>
<li><a href="#sec-2-6">2.6. Lecture 6 - Risk aversion and simple models of returns</a></li>
<li><a href="#sec-2-7">2.7. Lecture 7 - Forming sensible portfolios</a></li>
<li><a href="#sec-2-8">2.8. Lecture 8 - CAPM</a></li>
<li><a href="#sec-2-9">2.9. Lecture 9 - CAPM fit</a></li>
<li><a href="#sec-2-10">2.10. Lecture 10 - Model diagnostics</a></li>
<li><a href="#sec-2-11">2.11. Lecture 11 - Fixing OLS and EMH</a></li>
<li><a href="#sec-2-12">2.12. Lecture 12 - Stationary series</a></li>
<li><a href="#sec-2-13">2.13. Lecture 13 - AR models</a></li>
<li><a href="#sec-2-14">2.14. Lecture 14 - Modelling volatility</a></li>
<li><a href="#sec-2-15">2.15. Lecture 15 - ARCH moments</a></li>
<li><a href="#sec-2-16">2.16. Lecture 16 - GARCH models</a></li>
<li><a href="#sec-2-17">2.17. Lecture 17 - GARCH properties</a></li>
<li><a href="#sec-2-18">2.18. Lecture 18 - Asymmetric volatility</a></li>
<li><a href="#sec-2-19">2.19. Lecture 19 - Multivariate GARCH</a></li>
<li><a href="#sec-2-20">2.20. Lecture 20 -</a></li>
<li><a href="#sec-2-21">2.21. Lecture 21 - Review lecture</a></li>
</ul>
</li>
<li><a href="#sec-3">3. Laboratories</a>
<ul class="nav">
<li><a href="#sec-3-1">3.1. Week 2 laboratory</a></li>
<li><a href="#sec-3-2">3.2. Week 3 laboratory</a></li>
<li><a href="#sec-3-3">3.3. Week 4 laboratory</a></li>
<li><a href="#sec-3-4">3.4. Week 5 laboratory</a></li>
<li><a href="#sec-3-5">3.5. Week 6 laboratory</a></li>
<li><a href="#sec-3-6">3.6. Week 7 laboratory</a></li>
<li><a href="#sec-3-7">3.7. Week 8 laboratory</a></li>
<li><a href="#sec-3-8">3.8. Week 9 laboratory</a></li>
<li><a href="#sec-3-9">3.9. Week 10 laboratory</a></li>
<li><a href="#sec-3-10">3.10. Week 11 laboratory</a></li>
</ul>
</li>
</ul>
</div>
</nav>
</div></div></div>
<footer id="postamble" class="">
<div><p class="author">Author: Moss Ebeling - Semester 1 2019</p>
<p class="date">Created: 2019-06-06 Thu 12:18</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 26.2 (<a href="http://orgmode.org">Org-mode</a> 9.1.9)</p>
</div>
</footer>
</body>
</html>
