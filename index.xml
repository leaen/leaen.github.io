<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Moss&#39; blog</title>
    <link>https://banay.me/</link>
    <description>Recent content on Moss&#39; blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Moss Ebeling</copyright>
    <lastBuildDate>Sat, 14 Mar 2020 18:18:00 +0100</lastBuildDate>
    
	<atom:link href="https://banay.me/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Websockets with Elm using ports</title>
      <link>https://banay.me/websockets-in-elm/</link>
      <pubDate>Sat, 14 Mar 2020 18:18:00 +0100</pubDate>
      
      <guid>https://banay.me/websockets-in-elm/</guid>
      <description>Introduction Recently I came across the need for an easy UI to display information coming from a websocket. A niche option but one I&amp;rsquo;ve had a pleasant experience with previously is Elm, a functional language designed specifically for web applications.
Previously, the standard approach for connecting to a websocket was through the elm/websocket package, has been recently broken after some of the recent changes to the Elm language. In the mean time while the package and others are being updated, there&amp;rsquo;s a simple workaround by using another Elm feature called ports which allow for interop with JavaScript code.</description>
    </item>
    
    <item>
      <title>Let&#39;s write a Neural Arithmetic Logic Unit</title>
      <link>https://banay.me/nalu/</link>
      <pubDate>Tue, 14 Jan 2020 17:46:00 +1100</pubDate>
      
      <guid>https://banay.me/nalu/</guid>
      <description>Introduction A few months ago I read this paper from DeepMind that addressed a simple choice of architecture to encourage sensible weights in neural networks when solving problems that at the core are simple arithmetic. Despite the continued hype surrounding neural networks and deep learning in general, some simple problems like this are difficult to generalise past the regions used in a training set.
XOR was a famously difficult problem that stunted developments in perceptrons (the predecessors to what has become neural networks) until Marvin Minsky and Seymour Papert addressed it by applying composition to the model.</description>
    </item>
    
    <item>
      <title>Playing Tic-tac-toe with minimax in Python</title>
      <link>https://banay.me/tic-tac-toe-minimax/</link>
      <pubDate>Sun, 19 May 2019 14:36:00 +1000</pubDate>
      
      <guid>https://banay.me/tic-tac-toe-minimax/</guid>
      <description>Introduction In this article we will explain the minimax algorithm. We&amp;rsquo;ll cover game trees, the minimax algorithm itself and a simple implementation in Python. We&amp;rsquo;ll also review some popular extensions that speed up or improve upon the actions taken by minimax.
Game trees For games with perfect information, we can model the entire play-space using a directed graph called game tree. A game tree simply illustrates all possible ways in which a game may play out.</description>
    </item>
    
    <item>
      <title>Auto-regressive time series in R</title>
      <link>https://banay.me/auto-regressive-time-series-in-r/</link>
      <pubDate>Tue, 26 Feb 2019 11:48:00 +1100</pubDate>
      
      <guid>https://banay.me/auto-regressive-time-series-in-r/</guid>
      <description>Introduction In this post we&amp;rsquo;ll go over auto-regressive time series. What they are, what they look like and some properties they exhibit. Throughout the post we&amp;rsquo;ll use small snippets of R to plot processes and visualisations.
What are autoregressive time series? An auto-regressive time series is a stochastic process in which future values are modelled by a linear combination of some number of previous values of the same process. In other words, a series which is regressed on itself in order to find coefficients that relate its past values to its future values.</description>
    </item>
    
    <item>
      <title>Hyperparameter selection with T-tests</title>
      <link>https://banay.me/hyperparameter-t-test/</link>
      <pubDate>Sun, 11 Nov 2018 19:14:00 +0100</pubDate>
      
      <guid>https://banay.me/hyperparameter-t-test/</guid>
      <description>Introduction One of the most important steps in developing a model for machine learning is tuning hyperparameters to ensure it generalises to unseen data. A model that fits the training set well but performs poorly on anything else is useless, so care should be taken in ensuring that in-sample performance characteristics of a model are representative of real world performance also. The most simple of these methods is of course using a holdout set, which allows for an easy way to estimate performance on out-sample data but when used as part of the model iteration process, is usually is also overfit to some degree.</description>
    </item>
    
    <item>
      <title>Fast keyword matching with the Aho-Corasick algorithm</title>
      <link>https://banay.me/aho-corasick/</link>
      <pubDate>Wed, 22 Aug 2018 19:20:00 +1000</pubDate>
      
      <guid>https://banay.me/aho-corasick/</guid>
      <description>Introduction In this post we&amp;rsquo;ll look at the problem of keyword matching including a number of approaches, applications and the Aho-Corasick algorithm.
Statement of problem To begin, let&amp;rsquo;s define the keyword searching problem.
Given a list of strings \(K = [s_1, \ldots, s_n]\) (called keywords) and a (usually) much longer string \(C\) (called the corpus) count the number of times each keyword appears in the corpus.
Let \(|s_i|\) be the length of the \(i\) - th keyword and let \(m\) be the length of the corpus \(C\).</description>
    </item>
    
  </channel>
</rss>